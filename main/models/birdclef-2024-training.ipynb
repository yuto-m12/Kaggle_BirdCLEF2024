{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":8048860,"sourceType":"datasetVersion","datasetId":4745108},{"sourceId":8048895,"sourceType":"datasetVersion","datasetId":4746211},{"sourceId":8048897,"sourceType":"datasetVersion","datasetId":4746338},{"sourceId":8449682,"sourceType":"datasetVersion","datasetId":4963063},{"sourceId":8454200,"sourceType":"datasetVersion","datasetId":4963066},{"sourceId":8645952,"sourceType":"datasetVersion","datasetId":4963056}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":31.037946,"end_time":"2024-04-13T08:15:36.967243","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-13T08:15:05.929297","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"# Basic\nimport sys\nimport os\nimport gc\nimport copy\nimport yaml\nimport bz2\nimport pickle\nimport ast\nimport gzip\nimport random\nimport shutil\nfrom time import time\nimport typing as tp\nimport cv2\n\n# Python\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport pandas.api.types\nimport scipy\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport sklearn.metrics\nimport pywt\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport librosa\nimport librosa.display\n\n# Notebook\n# from tqdm.notebook import tqdm\nfrom tqdm import tqdm\nfrom IPython.display import Audio\n\n# Pytorch\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\nfrom torch.utils.data import Dataset\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n# use one device only\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# delete the display limit of columns\npd.set_option('display.max_columns', None)\n\n# type elements\nFilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:04:59.128334Z","iopub.execute_input":"2024-05-10T06:04:59.129203Z","iopub.status.idle":"2024-05-10T06:04:59.136923Z","shell.execute_reply.started":"2024-05-10T06:04:59.129159Z","shell.execute_reply":"2024-05-10T06:04:59.135846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    model_name = \"efficientnet_b0.ra_in1k\"  # model used\n    img_size = 224                          # input size. If it's 256, input image resize to 256x256\n    folds = 5                               # number of folds\n    interpolation = cv2.INTER_AREA          # specifying method of interpolation(default is cv2.INTER_LINEAR)\n    max_epoch = 9                           # number of max epoch.\n    batch_size = 32                         # batch size. Number of samples passed to the network in one training step\n    lr = 1.0e-03                            # learning rate. determine step size when updating model's weight\n    weight_decay = 1.0e-02                  # weight decay. Append regularization term for prevent over fitting\n    es_patience = 5                         # Early Stopping\n    seed = 1086                             # seed\n    deterministic = True                    # deterministic behaviour or not\n    enable_amp = False                      # Automatic Mixed Precision\n    device = \"cuda\"                         # device for training.\"cuda\" is NVIDIA GPU\n    simple_training = False                 # only use few data.\n    n_simple = 500                          # number of data with simple training\n    \n    KAGGLE_TRAIN = '/kaggle/input/birdclef-2024/train_audio'\n    ADDED_TRAIN_1 = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\n    ADDED_TRAIN_2 = '/kaggle/input/birdclef2024-additional-wav-2/additional_audio-2'\n    SAVE_TRAIN = '/kaggle/working/train_image'\n    TRAIN_IMAGE = Path('/kaggle/input/bird2024-spec-v6/train_image/spec')\n    make_dataset = False\n    \n    sample_submission = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv')\n    CLASSES = sample_submission.columns[1:].values\n    N_CLASSES = len(CLASSES)\n    \n    \nos.makedirs(CFG.KAGGLE_TRAIN, exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. 目標\n目標: **鳥の鳴き声の音声ファイルから、鳥の種類を判別/分類すること。**\n\n- 提出は184の鳥種を列名に持つcsvとして行うため、各行につき184要素の予測分布を作成する必要がある。  \n- テストデータには約1100個の4分間の音声データを使用する。\n- 音声データを区切った、各row_idが示す5秒間に対して予測を行う。\n","metadata":{"papermill":{"duration":0.003915,"end_time":"2024-04-13T08:15:08.543976","exception":false,"start_time":"2024-04-13T08:15:08.540061","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 0.1 評価指標\nマクロ平均ROC-AUCスコアを使用します。  \n[公式Notebook](https://www.kaggle.com/code/metric/birdclef-roc-auc/notebook)による以下のscore関数によって計算されます。\n\n・引数  \nsolution: 教師データ  \nsubmission: 推論データ  \nrow_id_column_name: 両データに共通する識別id列の列名","metadata":{"papermill":{"duration":0.003857,"end_time":"2024-04-13T08:15:08.552015","exception":false,"start_time":"2024-04-13T08:15:08.548158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# kaggle_metric_utilitiesが使用できないので実際に試すことはできません\nimport pandas as pd\nimport pandas.api.types\n\n# import kaggle_metric_utilities\n\nimport sklearn.metrics\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    '''\n    Version of macro-averaged ROC-AUC score that ignores all classes that have no true positive labels.\n    '''\n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n\n    if not pandas.api.types.is_numeric_dtype(submission.values):\n        bad_dtypes = {x: submission[x].dtype  for x in submission.columns if not pandas.api.types.is_numeric_dtype(submission[x])}\n        raise ParticipantVisibleError(f'Invalid submission data types found: {bad_dtypes}')\n\n    solution_sums = solution.sum(axis=0)\n    scored_columns = list(solution_sums[solution_sums > 0].index.values)\n    assert len(scored_columns) > 0\n\n    return kaggle_metric_utilities.safe_call_score(sklearn.metrics.roc_auc_score, solution[scored_columns].values, submission[scored_columns].values, average='macro')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.114317,"end_time":"2024-04-13T08:15:10.670372","exception":false,"start_time":"2024-04-13T08:15:08.556055","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.168993Z","iopub.execute_input":"2024-05-10T06:04:59.169826Z","iopub.status.idle":"2024-05-10T06:04:59.178385Z","shell.execute_reply.started":"2024-05-10T06:04:59.169797Z","shell.execute_reply":"2024-05-10T06:04:59.177547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"・試しにsample_submissionを提出","metadata":{"papermill":{"duration":0.004265,"end_time":"2024-04-13T08:15:10.679768","exception":false,"start_time":"2024-04-13T08:15:10.675503","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CFG.sample_submission.to_csv('submission.csv')\n# Score 0.5","metadata":{"papermill":{"duration":0.059181,"end_time":"2024-04-13T08:15:10.743448","exception":false,"start_time":"2024-04-13T08:15:10.684267","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.180228Z","iopub.execute_input":"2024-05-10T06:04:59.180942Z","iopub.status.idle":"2024-05-10T06:04:59.305399Z","shell.execute_reply.started":"2024-05-10T06:04:59.180906Z","shell.execute_reply":"2024-05-10T06:04:59.304716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. データ確認\n与えられたデータは以下の通りです。  \n・train_audio: 学習用データ    \n・test_soundscapes: テストデータが格納される  \n・unlabeled_soundscapes: テストデータと同じ場所で録音された、ラベルのない音声データ   \n・train_metadata.csv: 学習用データのメタデータ   \n・sample_submission.csv  \n・eBird_Taxonomy_v2021.csv: 鳥種間の関係性についてのデータ\n\nそれぞれについて解説します。\n- train_audio  \nディレクトリ構成は`train_audio/'bird_species_name'/'file_name'.ogg`で、テストデータに合わせて32KHzにダウンサンプリングされた音声データがoog形式で保存されています。  \n- test_soundscapes  \nNotebookが提出された時に、このディレクトリに4分間の約1100個の音声データが配置されます。ファイル名はランダムですが、`soundscape_xxxxxx.ogg`という一般的な名前です。  \n  全データのダウンロードに約5分かかります。  \n- unlabeled_soundscapes  \nテストデータと同じ場所で録音された、ラベルのついていない音声データです。  \nこれ以上の説明はありませんでした。事前学習に使えるかもしれません。  \n- train_metadata.csv  \n`train_audio`に関するメタデータが記述されています。主要な列の意味は以下の通りです。  \n・primary_label: 鳥種を識別するコード    \n例: `amecro`など。`https://ebird.org/species/amecro`のように、URLの末尾に指定するとより詳細な鳥種の情報を確認できます(幾つかのページは壊れています)。  \n・latitude & longitude: 録音した場所  \n一部の鳥種には'方言'があるため、地理的な多様性を確保することが推奨されます。  \n・author: 録音を行ったユーザー名  \n・filename: このメタデータが紐づけられたファイルの名前  \n- sample_submission.csv  \n有効な提出ファイルの例。列の意味は以下の通りです  \n・row_id: 予測に使用するid  \nrow_idは`soundscape_[soundscape_id]_[end_time]`の形式で指定される  \n・[bird_id]: 鳥種コードです。182個存在し、それぞれの鳥の確率を予測する必要があります。 \n- eBird_Taxonomy_v2021.csv\n鳥種間の関係性についてのデータです。\n\n<br>  \ntrain_metadata\n\ntrain_audio  \neBird_Taxonomy_v2021.csv     \n\nこれらのより詳細な確認は次で行います。    ","metadata":{"papermill":{"duration":0.004374,"end_time":"2024-04-13T08:15:10.75308","exception":false,"start_time":"2024-04-13T08:15:10.748706","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 1.1 train_metadata \n学習用データのメタデータついて、詳細を確認します。","metadata":{"papermill":{"duration":0.004328,"end_time":"2024-04-13T08:15:10.762062","exception":false,"start_time":"2024-04-13T08:15:10.757734","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# メタデータ\nmetadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\ndisplay(metadata.head(5))","metadata":{"papermill":{"duration":0.149003,"end_time":"2024-04-13T08:15:10.915718","exception":false,"start_time":"2024-04-13T08:15:10.766715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.306431Z","iopub.execute_input":"2024-05-10T06:04:59.3067Z","iopub.status.idle":"2024-05-10T06:04:59.444271Z","shell.execute_reply.started":"2024-05-10T06:04:59.306677Z","shell.execute_reply":"2024-05-10T06:04:59.443306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"各列について解説します。  \n・primary_label: 鳥種を識別するコード。例: `amecro`など  \n・secondary_label: ファイルに含まれる、中心的でない鳥種(紛れ込んだ鳥種)  \n・type: データの種類。[call, song, ]  \n・latitude: 録音した場所の緯度(南北)  \n・longitude: 録音した場所の経度(東西)  \n・scientific_name: 学名  \n・common_name: 一般名  \n・author: 録音を行ったユーザー名  \n・license: ライセンス  \n・rating: xenocantによる録音品質の評価。 範囲は[1,5]で、0は未評価を示す  \n・url: xeno-canto.orgのデータurl  \n・filename: このメタデータが紐づけられたファイルの名前  \n\n次は統計データを確認します。","metadata":{"papermill":{"duration":0.00453,"end_time":"2024-04-13T08:15:10.925365","exception":false,"start_time":"2024-04-13T08:15:10.920835","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# メタデータの統計データ\nprint('\\n'+ '-'*15 +' '+ f'shape' +' '+ '-'*15)\nprint(metadata.shape)\nprint('\\n'+ '-'*15 +' '+ f'columns' +' '+ '-'*15)\nprint(metadata.columns)\nprint('\\n'+ '-'*15 +' '+ f'info' +' '+ '-'*15)\ndisplay(metadata.info())\nprint('\\n'+ '-'*15 +' '+ f'null' +' '+ '-'*15)\ndisplay(metadata.isnull().sum())\nprint('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\ndisplay(metadata.describe())\nprint('\\n'+ '-'*15 +' '+ f'value_counts' +' '+ '-'*15)\n\ncount_column = ['primary_label', 'type', 'author', 'rating']\nfor column in count_column:\n    print('\\n')\n    display(metadata[column].value_counts())","metadata":{"papermill":{"duration":0.108152,"end_time":"2024-04-13T08:15:11.038458","exception":false,"start_time":"2024-04-13T08:15:10.930306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.445552Z","iopub.execute_input":"2024-05-10T06:04:59.445855Z","iopub.status.idle":"2024-05-10T06:04:59.549443Z","shell.execute_reply.started":"2024-05-10T06:04:59.445829Z","shell.execute_reply":"2024-05-10T06:04:59.548504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### And there are so many types. \nWhen use it, We have to handle them appropriately.","metadata":{"papermill":{"duration":0.006186,"end_time":"2024-04-13T08:15:11.052858","exception":false,"start_time":"2024-04-13T08:15:11.046672","status":"completed"},"tags":[]}},{"cell_type":"code","source":"display(metadata['type'].value_counts())\n\nunique_types = []\nfor type_list in metadata['type']:\n    for type_elem in ast.literal_eval(type_list):\n        if not type_elem in unique_types:\n            unique_types.append(type_elem)\n\nprint('\\n unique_types: ',unique_types)","metadata":{"papermill":{"duration":0.360618,"end_time":"2024-04-13T08:15:11.420323","exception":false,"start_time":"2024-04-13T08:15:11.059705","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.552517Z","iopub.execute_input":"2024-05-10T06:04:59.552855Z","iopub.status.idle":"2024-05-10T06:04:59.840436Z","shell.execute_reply.started":"2024-05-10T06:04:59.55283Z","shell.execute_reply":"2024-05-10T06:04:59.839505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**また各データ数が500で打ち止めとなっているため、正しいデータを取得する必要があります。**  \nShiroさんが追加のデータセットを公開してくれています。","metadata":{"papermill":{"duration":0.006708,"end_time":"2024-04-13T08:15:11.434918","exception":false,"start_time":"2024-04-13T08:15:11.42821","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 1.2 train_audio","metadata":{"papermill":{"duration":0.006372,"end_time":"2024-04-13T08:15:11.448076","exception":false,"start_time":"2024-04-13T08:15:11.441704","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"##### 1.2.1 check data (include additional data)","metadata":{"papermill":{"duration":0.006452,"end_time":"2024-04-13T08:15:11.461441","exception":false,"start_time":"2024-04-13T08:15:11.454989","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom IPython.display import Audio\n\n\nmetadata_df = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\ntrain_path = '/kaggle/input/birdclef-2024/train_audio/'\ndata, rate = librosa.load(train_path + metadata_df.filename[0])\nprint('data shape: ', data.shape)\nprint('sampling rate: ', rate)\nprint(type(data))\n\ndisplay(Audio(data[:rate*5], rate=rate)) # choice first channel and first 5s (rate: num of samples per second)\ndisplay(px.line(y=data[:rate*5], title=metadata_df.common_name[0]))\n\nmetadata_adwav = pd.read_csv('/kaggle/input/birdclef2024-additional-wav-1/BirdClef2024_additional.csv')\ntrain_path_adwav = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\ndata, rate = librosa.load(train_path_adwav + '/asbfly/XC155673.wav')\ndisplay(Audio(data[:rate*5], rate=rate))\ndisplay(px.line(y=data[:rate*5]))","metadata":{"papermill":{"duration":9.775946,"end_time":"2024-04-13T08:15:21.2442","exception":false,"start_time":"2024-04-13T08:15:11.468254","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:04:59.841507Z","iopub.execute_input":"2024-05-10T06:04:59.841783Z","iopub.status.idle":"2024-05-10T06:05:00.730059Z","shell.execute_reply.started":"2024-05-10T06:04:59.841759Z","shell.execute_reply":"2024-05-10T06:05:00.728934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 1.2.2 check MFCC","metadata":{"papermill":{"duration":0.056186,"end_time":"2024-04-13T08:15:21.361009","exception":false,"start_time":"2024-04-13T08:15:21.304823","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\n\n# Load an audio file\ny_ogg, sr_ogg = librosa.load('/kaggle/input/birdclef-2024/train_audio/asbfly/XC134896.ogg')\ny_wav, sr_wav = librosa.load('/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1/asbfly/XC155673.wav')\n\n# Extract MFCC\nmfcc_ogg = librosa.feature.mfcc(y=y_ogg, sr=sr_ogg)\nmfcc_wav = librosa.feature.mfcc(y=y_wav, sr=sr_wav)\nmfccs = {'ogg':mfcc_ogg, 'wav':mfcc_wav}\n\n# Plot the MFCCs\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\naxes = axes.flatten()\n\nfor i, mfcc_key in enumerate(mfccs):\n    ax = axes[i]\n    print(ax)\n    print(type(ax))\n    img = librosa.display.specshow(mfccs[mfcc_key], x_axis='time', ax=ax)\n    cbar = fig.colorbar(img, ax=ax)\n    cbar.set_label('Value')\n    ax.set_title('MFCC_'+f'{mfcc_key}')\n\nplt.tight_layout()   \nplt.show()\n","metadata":{"papermill":{"duration":12.055665,"end_time":"2024-04-13T08:15:33.470445","exception":false,"start_time":"2024-04-13T08:15:21.41478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:05:00.731505Z","iopub.execute_input":"2024-05-10T06:05:00.732194Z","iopub.status.idle":"2024-05-10T06:05:01.744828Z","shell.execute_reply.started":"2024-05-10T06:05:00.732159Z","shell.execute_reply":"2024-05-10T06:05:01.743885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 eBird_Taxonomy_v2021.csv\nこのファイルは分類学的情報を持ちます。","metadata":{"papermill":{"duration":0.05274,"end_time":"2024-04-13T08:15:33.581056","exception":false,"start_time":"2024-04-13T08:15:33.528316","status":"completed"},"tags":[]}},{"cell_type":"code","source":"taxonomy = pd.read_csv('/kaggle/input/birdclef-2024/eBird_Taxonomy_v2021.csv')\n\ndisplay(taxonomy.head())\nprint('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\ndisplay(taxonomy.describe())\ndisplay(taxonomy.describe(include=['O']))\n","metadata":{"papermill":{"duration":0.182924,"end_time":"2024-04-13T08:15:33.816548","exception":false,"start_time":"2024-04-13T08:15:33.633624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T06:05:01.746178Z","iopub.execute_input":"2024-05-10T06:05:01.746541Z","iopub.status.idle":"2024-05-10T06:05:01.884005Z","shell.execute_reply.started":"2024-05-10T06:05:01.746509Z","shell.execute_reply":"2024-05-10T06:05:01.883099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"各列について解説します。<br><br>\nTAXON_ORDER: 鳥種の分類番号   \nCATEGORY: 鳥種  \nSPECIES_CODE: 鳥種コード  \nPRIMARY_COM_NAME: 鳥種の一般名\nSCI_NAME: 鳥種の学名  \nORDER1: 何目か(イヌ目など)  \nFAMILY: 分類学上の家族  \nSPECIES_GROUP: 種族グループ  \nREPORT_AS: 鳥種に関する追加情報  \n\n","metadata":{"papermill":{"duration":0.059472,"end_time":"2024-04-13T08:15:33.926745","exception":false,"start_time":"2024-04-13T08:15:33.867273","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 2. Create data and Save","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Load kaggle train metadata","metadata":{}},{"cell_type":"code","source":"# Load kaggle train metadata\nTRAIN_CSV = '/kaggle/input/birdclef-2024/train_metadata.csv'\ntrain_csv_df = pd.read_csv(TRAIN_CSV)\nprint(train_csv_df.columns)\n\nfilename_df = train_csv_df['filename']","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:01.891137Z","iopub.execute_input":"2024-05-10T06:05:01.89143Z","iopub.status.idle":"2024-05-10T06:05:01.99921Z","shell.execute_reply.started":"2024-05-10T06:05:01.891406Z","shell.execute_reply":"2024-05-10T06:05:01.998236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Load metadata added","metadata":{}},{"cell_type":"code","source":"# Load metadata added\nTRAIN_CSV_ADD = '/kaggle/input/birdclef2024-additional-wav-1/BirdClef2024_additional.csv'\ntrain_csv_add_df = pd.read_csv(TRAIN_CSV_ADD)\n\nprint(train_csv_add_df.columns)\ndisplay(train_csv_add_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:02.00049Z","iopub.execute_input":"2024-05-10T06:05:02.00091Z","iopub.status.idle":"2024-05-10T06:05:02.479272Z","shell.execute_reply.started":"2024-05-10T06:05:02.000849Z","shell.execute_reply":"2024-05-10T06:05:02.478345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Aligne metadata added","metadata":{}},{"cell_type":"code","source":"# Aligne metadata added\npd.set_option('future.no_silent_downcasting', True)\ndef Aligne_metadata_added(df:pd.DataFrame):\n    if 'lat' in df.columns:\n        # Adjust kaggle_train and additional_train for merge, columns required the processing: file, q , type\n        alignment_dict = {'latitude':'lat', 'longitude':'lng', 'filename': 'file', 'rating':'q'}\n        in_alingment_dict = {value: key for key, value in alignment_dict.items()}\n\n        # rename\n        df.rename(columns=in_alingment_dict, inplace=True)\n        # modify filename to align with kaggle train\n        df['filename'] = df['primary_label'] + '/' + df['filename'] + '.wav'\n        # modify q\n        dict_q_to_rating = {'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'no score':0}\n        df['rating'] = df['rating'].replace(dict_q_to_rating)\n        df['rating'] = df['rating'].astype('int64')\n        # modify train\n        df['type'] = \"[\\'\" + df['type'].str.replace(\", \", \"\\', \\'\") + \"\\']\"\n        df['added'] = True\n    return df\n\n\n# check df aligned\ntrain_csv_add_df_aligned = Aligne_metadata_added(train_csv_add_df)\ntrain_csv_add_df_aligned.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:02.480538Z","iopub.execute_input":"2024-05-10T06:05:02.480855Z","iopub.status.idle":"2024-05-10T06:05:02.568045Z","shell.execute_reply.started":"2024-05-10T06:05:02.480828Z","shell.execute_reply":"2024-05-10T06:05:02.567231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Concat kaggle metadata and added","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n# check shape\nprint('before: ')\nprint(train_csv_df.shape)\nprint(train_csv_add_df_aligned.shape)\n\n# concat\ntrain_csv_all_df = pd.concat([train_csv_df, train_csv_add_df_aligned], axis=0).reset_index(drop=True)\n# add column of stem (ex. XC134896)\ntrain_csv_all_df['stem'] = train_csv_all_df['filename'].apply(lambda x: Path(x).stem)\n\n# remove not exist data(because because of the link being dead on the website)\nnot_exist_list = [\n    'aspfly1/XC775312',\n    'comior1/XC881009',\n    'hoopoe/XC891005',\n    'hoopoe/XC891004',\n    'hoopoe/XC798809',\n    'hoopoe/XC798808',\n    'hoopoe/XC798807',\n    'hoopoe/XC798806',\n    'hoopoe/XC798805',\n    'eaywag1/XC835367',\n    'orihob2/XC762524',\n]\nfor i, filename in enumerate(not_exist_list):\n    not_exist_list[i] = filename + '.wav'\n\n# Filter the DataFrame\ntrain_csv_all_df = train_csv_all_df[~train_csv_all_df['filename'].isin(not_exist_list)]\ntrain_csv_all_df = train_csv_all_df.reset_index()\n\n# check shape and content\nprint('after concat: ')\nprint(train_csv_all_df.shape)\ntrain_csv_all_df","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:02.583667Z","iopub.execute_input":"2024-05-10T06:05:02.583995Z","iopub.status.idle":"2024-05-10T06:05:03.178253Z","shell.execute_reply.started":"2024-05-10T06:05:02.58397Z","shell.execute_reply":"2024-05-10T06:05:03.177279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check statistics","metadata":{}},{"cell_type":"code","source":"import ast\n\n# メタデータの統計データ\nprint('\\n'+ '-'*15 +' '+ f'shape' +' '+ '-'*15)\nprint(train_csv_all_df.shape)\nprint('\\n'+ '-'*15 +' '+ f'columns' +' '+ '-'*15)\nprint(train_csv_all_df.columns)\nprint('\\n'+ '-'*15 +' '+ f'info' +' '+ '-'*15)\ndisplay(train_csv_all_df.info())\nprint('\\n'+ '-'*15 +' '+ f'null' +' '+ '-'*15)\ndisplay(train_csv_all_df.isnull().sum())\nprint('\\n'+ '-'*15 +' '+ f'nunique of stem' +' '+ '-'*15)\nprint(train_csv_all_df['stem'].nunique())\nprint('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\ndisplay(train_csv_all_df.describe())\nprint('\\n'+ '-'*15 +' '+ f'value_counts' +' '+ '-'*15)\n\ncount_column = ['primary_label', 'type', 'author', 'rating']\nfor column in count_column:\n    print('\\n')\n    display(train_csv_all_df[column].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:03.179514Z","iopub.execute_input":"2024-05-10T06:05:03.179818Z","iopub.status.idle":"2024-05-10T06:05:03.521461Z","shell.execute_reply.started":"2024-05-10T06:05:03.179793Z","shell.execute_reply":"2024-05-10T06:05:03.520437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Split Fold","metadata":{}},{"cell_type":"markdown","source":"### 3.1 split fold","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef split_fold(df:pd.DataFrame):\n    # config\n    N_FOLDS = 5\n    RANDAM_SEED = 42\n    df['fold'] = -1\n\n    # object\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\n\n    for i, (train_index, test_index) in enumerate(skf.split(df, df['primary_label'])):\n        df.loc[test_index, 'fold'] = i\n    \n    return df\n        \ntrain_csv_all_df = split_fold(train_csv_all_df)\n\ndisplay(train_csv_all_df.head())\ntrain_csv_all_df.to_csv('train_csv_all.csv', index=False)\n\n# check\nview_data = False\nif view_data:\n    with pd.option_context('display.max_rows', 300):\n        print(train_csv_all_df.groupby('fold')['primary_label'].value_counts().head(300))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:04.407549Z","iopub.execute_input":"2024-05-10T06:05:04.407987Z","iopub.status.idle":"2024-05-10T06:05:06.435492Z","shell.execute_reply.started":"2024-05-10T06:05:04.407954Z","shell.execute_reply":"2024-05-10T06:05:06.434414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Pre-Processing\n##### Make Dataset in here.\n\nThis is needed for using when inference.","metadata":{}},{"cell_type":"code","source":"if CFG.make_dataset:\n    class preprocessing():\n        def __init__(self, AUDIO_DIRECTORY, SAVE_DIRECTORY, view=False):\n            # config\n            self.AUDIO_DIRECTORY = AUDIO_DIRECTORY\n            self.SAVE_DIRECTORY = SAVE_DIRECTORY\n            self.view = view\n\n            # make directory\n            func_names = [method for method in dir(self) if callable(getattr(self, method)) and method.startswith(\"func\")]\n            print(func_names)\n            os.makedirs(self.SAVE_DIRECTORY, exist_ok=True)\n            for func_name in func_names:\n                func = func_name.split('_')[-1]\n                os.makedirs(self.SAVE_DIRECTORY + '/' + func, exist_ok=True)\n\n        # apply and save\n        def apply_func(self, function):\n            train_dict = {}\n            species_list = os.listdir(self.AUDIO_DIRECTORY)\n            for species in species_list:\n                species_path = self.AUDIO_DIRECTORY + '/' + species\n                audio_file_list = os.listdir(species_path)\n                for audio_file in audio_file_list:\n                    audio_filepath = species_path +'/' + audio_file\n                    self.load_wave(audio_filepath) # load audio\n                    mfcc = function() # apply function\n                    train_dict[audio_file.split(\".\")[0]] = mfcc # register to dict\n\n            # set function name as filepath name\n            SAVE_PATH = self.SAVE_DIRECTORY + '/' + function.__name__.split('_')[-1] + f'/train.pickle.gz'\n            self.save_as_picke_gzip(train_dict, SAVE_PATH)\n\n        def save_as_picke_gzip(self, data, filepath):       \n            with gzip.open(filepath, 'wb') as f:\n                pickle.dump(data, f)\n\n        def load_wave(self, audio_filepath):\n            # pick up first 5 seconds\n            self.y, self.sr = librosa.load(audio_filepath, offset=0, duration=5)\n\n        def func_waveform(self):        \n            if self.view:\n                print('waveform shape: ', self.y.shape)\n                display(Audio(self.y, rate=self.sr))\n                plt.figure(figsize=(10, 4))\n                librosa.display.waveshow(self.y, sr=self.sr)\n                plt.title('Waveform')\n                plt.xlabel('Time (s)')\n                plt.ylabel('Amplitude')\n                plt.show()\n            return self.y\n\n        def func_spec(self):\n            spec = librosa.amplitude_to_db(np.abs(librosa.stft(self.y)), ref=np.max)\n\n            if self.view:\n                print('spec shape: ', spec.shape)\n                plt.figure(figsize=(10, 4))\n                librosa.display.specshow(spec, sr=self.sr, x_axis='time', y_axis='log')\n                plt.colorbar(format='%+2.0f dB')\n                plt.title('Spectrogram')\n                plt.show()\n            return spec\n\n        def func_melspec(self):\n            melspec = librosa.feature.melspectrogram(y=self.y, sr=self.sr, n_mels=128)\n            melspec_DB = librosa.power_to_db(melspec, ref=np.max)\n\n            if self.view:\n                print('melspec shape: ', melspec_DB.shape)\n                plt.figure(figsize=(10, 4))\n                librosa.display.specshow(melspec_DB, sr=self.sr, x_axis='time', y_axis='mel')\n                plt.colorbar(format='%+2.0f dB')\n                plt.title('Mel Spectrogram')\n                plt.show()\n            return melspec_DB\n\n        # on making\n        def func_scalogram(self):\n            scales = pywt.central_frequency('cmor') / np.linspace(1, 100, 100) * self.sr\n            cwtmatr, freqs = pywt.cwt(self.y, scales, 'cmor', sampling_period=1/self.sr)\n\n            if self.view:\n                print('scarogram shape: ', cwtmatr.shape)\n                plt.figure(figsize=(10, 4))\n                plt.imshow(abs(cwtmatr), aspect='auto', extent=[0, len(self.y) / self.sr, 1, 100], cmap='jet', origin='lower')\n                plt.colorbar()\n                plt.title('Scalogram')\n                plt.xlabel('Time (s)')\n                plt.ylabel('Scale')\n                plt.show()\n            # to real value\n            return abs(cwtmatr)\n\n        def func_chromagram(self):\n            C = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)\n\n            if self.view:\n                print('chromagram shape: ', C.shape)\n                plt.figure(figsize=(10, 4))\n                librosa.display.specshow(C, sr=self.sr, x_axis='time', y_axis='chroma', cmap='coolwarm')\n                plt.colorbar()\n                plt.title('Chromagram')\n                plt.show()\n            return C\n\n\n        def func_mfcc(self): \n            mfcc = librosa.feature.mfcc(y=self.y, sr=self.sr)\n\n            if self.view:\n                print('mfcc shape: ', mfcc.shape)\n                plt.figure(figsize=(10, 4))\n                librosa.display.specshow(mfcc, sr=self.sr, x_axis='time')\n                plt.ylabel('MFCC coeffs')\n                plt.colorbar()\n                plt.title('MFCC')\n                plt.show()\n            return mfcc\n\n        def func_spectralcontrast(self):\n            contrast = librosa.feature.spectral_contrast(y=self.y, sr=self.sr)\n\n            if self.view:\n                print('contrast shape: ', contrast.shape)\n                plt.figure(figsize=(10, 4))\n                librosa.display.specshow(contrast, x_axis='time')\n                plt.colorbar()\n                plt.ylabel('Frequency bands')\n                plt.title('Spectral Contrast')\n                plt.show()\n            return contrast\n\n\n        def execute(self):\n            func_list = [\n    #             self.func_waveform,\n    #             self.func_spec,\n    #             self.func_melspec,\n                self.func_scalogram,\n    #             self.func_chromagram,\n    #             self.func_mfcc,\n    #             self.func_spectralcontrast,\n            ]\n            for func in func_list:\n                self.apply_func(func)\n\n    # ・ Define preprocessing class\n    preprocessing_kaggle = preprocessing(CFG.KAGGLE_TRAIN, CFG.SAVE_TRAIN)\n    preprocessing_added_train_1 = preprocessing(CFG.ADDED_TRAIN_1, CFG.SAVE_TRAIN)\n    preprocessing_added_train_2 = preprocessing(CFG.ADDED_TRAIN_2, CFG.SAVE_TRAIN)\n\n    # ・ Execute preprocessing\n    preprocessing_kaggle.execute()\n    preprocessing_added_train_1.execute()\n    preprocessing_added_train_2.execute()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:06.484468Z","iopub.execute_input":"2024-05-10T06:05:06.484743Z","iopub.status.idle":"2024-05-10T06:05:06.517671Z","shell.execute_reply.started":"2024-05-10T06:05:06.484719Z","shell.execute_reply":"2024-05-10T06:05:06.516922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Create Dataset","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Settings","metadata":{}},{"cell_type":"code","source":"if CFG.simple_training:\n    train_csv_all_df = train_csv_all_df[:CFG.n_simple]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:06.626678Z","iopub.execute_input":"2024-05-10T06:05:06.627018Z","iopub.status.idle":"2024-05-10T06:05:06.672683Z","shell.execute_reply.started":"2024-05-10T06:05:06.626985Z","shell.execute_reply":"2024-05-10T06:05:06.67181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_data():\n    spec = np.load('/kaggle/input/bird2024-spec-v6/train_image/spec/ashdro1/XC100886.npy')\n\n    print(spec.shape)\n\n    print('spec shape: ', spec.shape)\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(spec, sr=32000, x_axis='time',)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n    plt.show()\n\n    # 2x2のサブプロットを作成\n    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n\n    ax = axes\n    # ランダムなデータを生成\n    data = np.random.rand(10, 10)\n    # サブプロットに画像を描画\n    img = ax.imshow(spec, cmap='viridis')\n    # カラーバーの追加\n    cbar = fig.colorbar(img, ax=ax)\n    cbar.set_label('Value')\n    # タイトルを追加\n    ax.set_title(f'Image at {i}')\n    # 軸の目盛りを非表示\n    ax.axis('off')\n\n    # サブプロット間のスペースを調整\n    plt.tight_layout()\n    plt.show()\n    \nif CFG.show:\n    show_data()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:06.701399Z","iopub.execute_input":"2024-05-10T06:05:06.701732Z","iopub.status.idle":"2024-05-10T06:05:07.578506Z","shell.execute_reply.started":"2024-05-10T06:05:06.7017Z","shell.execute_reply":"2024-05-10T06:05:07.577633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Utility Functions","metadata":{}},{"cell_type":"code","source":"def get_path_label(val_fold, train_csv_all_df: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    train_idx = train_csv_all_df[train_csv_all_df[\"fold\"] != val_fold].index.values\n    val_idx   = train_csv_all_df[train_csv_all_df[\"fold\"] == val_fold].index.values\n    img_paths = []\n    labels = train_csv_all_df['primary_label'].values\n    for filename in train_csv_all_df[\"filename\"].values:\n        img_path = CFG.TRAIN_IMAGE / filename.replace(\".ogg\", \".npy\").replace(\".wav\", \".npy\")\n        img_paths.append(img_path)\n\n    train_data = {\n        \"image_paths\": [img_paths[idx] for idx in train_idx],\n        \"labels\": [labels[idx] for idx in train_idx]}\n\n    val_data = {\n        \"image_paths\": [img_paths[idx] for idx in val_idx],\n        \"labels\": [labels[idx] for idx in val_idx]}\n    \n    return train_data, val_data, train_idx, val_idx\n\ntrain_data = get_path_label(0, train_csv_all_df)\n\n    \ndef get_transforms(CFG):\n    train_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n        ToTensorV2(p=1.0)\n    ])\n    val_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n        ToTensorV2(p=1.0)\n    ])\n    return train_transform, val_transform\n\ndef make_corresponding_list():\n    \"\"\"\n    return: list of train_path, list of label_path\n    # ID: XC000000,...\n    # LABEL: asbfly, ashdro1,...\n    # NUMBER: 0,1,...\n    \"\"\"\n    # make dict label to num (encoding)\n    LABEL2NUM = {}\n    for i, _class in enumerate(CFG.CLASSES):\n        LABEL2NUM[f'{_class}'] = i\n        NUM2LABEL = {value: key for key, value in LABEL2NUM.items()}\n\n    # make list of train path\n    train_path_list = []\n    ID_list = []\n    # train_image dir\n    print('Making train_path_list...')\n    for species in CFG.TRAIN_IMAGE.iterdir():\n        # image file\n        for image in species.iterdir():\n            if not CFG.simple_training: # normal training\n                train_path_list.append(str(image.absolute()))\n                ID_list.append(str(image.stem))\n            else: # simple training\n                if image.stem in train_csv_all_df['stem']:\n                    train_path_list.append(str(image.absolute()))\n                    ID_list.append(str(image.stem))                \n            \n    print('Making label_list...')\n    # make dict corresponding label and stem\n    ID2LABEL = pd.Series(train_csv_all_df.primary_label.values, index=train_csv_all_df.stem).to_dict()\n    # If LABEL2NUM is already defined mapping 'primary_label' to a number:\n    ID2LABEL2NUM = {ID: LABEL2NUM[label] for ID, label in ID2LABEL.items()}\n    # make label_list\n    label_list = [ID2LABEL2NUM[key] for key in ID_list]\n\n    return LABEL2NUM\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:07.579887Z","iopub.execute_input":"2024-05-10T06:05:07.580227Z","iopub.status.idle":"2024-05-10T06:05:07.599453Z","shell.execute_reply.started":"2024-05-10T06:05:07.580197Z","shell.execute_reply":"2024-05-10T06:05:07.598524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seed\ndef set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n# set tensor to device\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:07.600457Z","iopub.execute_input":"2024-05-10T06:05:07.600721Z","iopub.status.idle":"2024-05-10T06:05:07.613596Z","shell.execute_reply.started":"2024-05-10T06:05:07.600698Z","shell.execute_reply":"2024-05-10T06:05:07.612659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Dataset","metadata":{}},{"cell_type":"code","source":"class Bird2024Dataset(Dataset):\n    def __init__(\n        self,\n        image_paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        self.train_path_list = image_paths\n        self.label_list = labels\n        self.transform = transform\n        \n    def __len__(self):\n        # return total num of data\n        return len(self.train_path_list)\n    \n    def __getitem__(self, index:int):\n        # return data and target assosiated with index\n        X = np.load(self.train_path_list[index])\n        X = self._apply_transform(X)\n        y = self.label_list[index]\n        y = LABEL2NUM[y]\n\n        return (X, y)\n    \n    def _apply_transform(self, img:np.ndarray):\n        \"\"\"apply transform to image\"\"\"\n        transformed = self.transform(image=img)\n        img = transformed[\"image\"].float()# .half()\n        return img","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:07.614721Z","iopub.execute_input":"2024-05-10T06:05:07.615008Z","iopub.status.idle":"2024-05-10T06:05:08.314613Z","shell.execute_reply.started":"2024-05-10T06:05:07.614985Z","shell.execute_reply":"2024-05-10T06:05:08.313455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Data","metadata":{}},{"cell_type":"code","source":"def show_batch(ds, row=3, col=3):\n    fig = plt.figure(figsize=(10, 10))\n    img_index = np.random.randint(0, len(ds)-1, row*col)\n    \n    for i in range(len(img_index)):\n        img, label = ds[img_index[i]]\n        \n        if isinstance(img, torch.Tensor):\n            img = img.detach().numpy()\n            img = np.squeeze(img)\n        \n        ax = fig.add_subplot(row, col, i + 1, xticks=[], yticks=[])\n        ax.imshow(img, cmap='jet')\n        ax.set_title(f'ID: {img_index[i]}; Target: {label}')\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef execute_show_batch():\n    _train_path_label, _val_path_label, _, _ = get_path_label(0, train_csv_all_df)\n    _train_transform, _val_transform = get_transforms(CFG)\n\n    _train_dataset = Bird2024Dataset(**_train_path_label, transform=_train_transform)\n\n    test_input, test_target = _train_dataset[0]\n    show_batch(_train_dataset)\n\nif CFG.show:\n    execute_show_batch()\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Define Model","metadata":{}},{"cell_type":"code","source":"class BirdCLEF2024SpecModel(nn.Module):\n\n    def __init__(\n            self,\n            model_name: str,\n            pretrained: bool,\n            in_channels: int,\n            num_classes: int,\n        ):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name=model_name, \n            pretrained=pretrained,\n            num_classes=num_classes, \n            in_chans=in_channels\n        )\n\n    def forward(self, x):\n        h = self.model(x)      \n\n        return h\n\nmodel = BirdCLEF2024SpecModel(model_name='efficientnet_b0.ra_in1k', pretrained=True, num_classes=CFG.N_CLASSES, in_channels=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:08.341618Z","iopub.execute_input":"2024-05-10T06:05:08.341897Z","iopub.status.idle":"2024-05-10T06:05:08.588828Z","shell.execute_reply.started":"2024-05-10T06:05:08.341857Z","shell.execute_reply":"2024-05-10T06:05:08.587802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Training","metadata":{}},{"cell_type":"code","source":"def train_one_fold(CFG, val_fold, train_all, output_path):\n    \"\"\"Main\"\"\"\n    # If True, forces cuDNN to benchmark multiple convolution algorithms and choose the fastest one\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(CFG.seed, deterministic=CFG.deterministic)\n    # set device with pytorch env\n    device = torch.device(CFG.device)\n    \n    # wiriting.....\n    train_path_label, val_path_label, _, _ = get_path_label(val_fold, train_csv_all_df)\n    train_transform, val_transform = get_transforms(CFG)\n    \n    train_dataset = Bird2024Dataset(**train_path_label, transform=train_transform)\n    val_dataset = Bird2024Dataset(**val_path_label, transform=val_transform)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=True, drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n    \n    model = BirdCLEF2024SpecModel(\n        model_name=CFG.model_name, \n        pretrained=True, \n        num_classes=CFG.N_CLASSES, \n        in_channels=1\n    )\n    model.to(device)\n    \n    optimizer = optim.AdamW(params=model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    scheduler = lr_scheduler.OneCycleLR(\n        optimizer=optimizer, epochs=CFG.max_epoch,\n        pct_start=0.0, steps_per_epoch=len(train_loader),\n        max_lr=CFG.lr, div_factor=25, final_div_factor=4.0e-01\n    )\n    \n    loss_func = nn.CrossEntropyLoss()\n    loss_func.to(device)\n    loss_func_val = nn.CrossEntropyLoss()\n    \n    use_amp = CFG.enable_amp\n    scaler = amp.GradScaler(enabled=use_amp)\n    \n    best_val_loss = 1.0e+09\n    best_epoch = 0\n    train_loss = 0\n    val_loss = 0\n    \n    for epoch in range(1, CFG.max_epoch + 1):\n        epoch_start = time()\n        model.train()\n        for batch in train_loader:\n            \n            x, t = batch\n            x = to_device(x, device)\n            t = to_device(t, device)\n                \n            optimizer.zero_grad()\n            with amp.autocast(use_amp):\n                y = model(x)\n                loss = loss_func(y, t)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            train_loss += loss.item()\n            scheduler.step()\n            \n        train_loss /= len(train_loader)\n            \n        model.eval()\n        for batch in val_loader:\n            x, t = batch\n            x = to_device(x, device)\n            with torch.no_grad(), amp.autocast(use_amp):\n                y = model(x)\n            y = y.detach().cpu().to(torch.float32)\n            loss = loss_func_val(y, t)\n            val_loss += loss.item()\n        val_loss /= len(val_loader)\n        \n        if val_loss < best_val_loss:\n            best_epoch = epoch\n            best_val_loss = val_loss\n            # print(\"save model\")\n            torch.save(model.state_dict(), str(output_path / f'snapshot_epoch_{epoch}.pth'))\n        \n        elapsed_time = time() - epoch_start\n        print(\n            f\"[epoch {epoch}] train loss: {train_loss: .6f}, val loss: {val_loss: .6f}, elapsed_time: {elapsed_time: .3f}\")\n        \n        if epoch - best_epoch > CFG.es_patience:\n            print(\"Early Stopping!\")\n            break\n            \n        train_loss = 0\n        val_loss = 0\n            \n    return val_fold, best_epoch, best_val_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:10.073568Z","iopub.execute_input":"2024-05-10T06:05:10.073868Z","iopub.status.idle":"2024-05-10T06:05:10.091796Z","shell.execute_reply.started":"2024-05-10T06:05:10.073842Z","shell.execute_reply":"2024-05-10T06:05:10.090771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_list = []\nfor fold_id in range(CFG.folds):\n    output_path = Path(f\"fold{fold_id}\")\n    output_path.mkdir(exist_ok=True)\n    print(f\"[fold{fold_id}]\")\n    score_list.append(train_one_fold(CFG, fold_id, train_csv_all_df, output_path))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:05:10.09338Z","iopub.execute_input":"2024-05-10T06:05:10.093735Z","iopub.status.idle":"2024-05-10T06:06:56.681168Z","shell.execute_reply.started":"2024-05-10T06:05:10.093702Z","shell.execute_reply":"2024-05-10T06:06:56.67994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Validation","metadata":{}},{"cell_type":"markdown","source":"##### check the results","metadata":{}},{"cell_type":"code","source":"print(score_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:06:56.684102Z","iopub.execute_input":"2024-05-10T06:06:56.684842Z","iopub.status.idle":"2024-05-10T06:06:56.690151Z","shell.execute_reply.started":"2024-05-10T06:06:56.684803Z","shell.execute_reply":"2024-05-10T06:06:56.689199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### select the best model and delete others","metadata":{}},{"cell_type":"code","source":"# select the best model and delete others\nbest_log_list = []\nfor (fold_id, best_epoch, _) in score_list:\n    \n    # select the best model\n    exp_dir_path = Path(f\"fold{fold_id}\")\n    best_model_path = exp_dir_path / f\"snapshot_epoch_{best_epoch}.pth\"\n    # copy to new place\n    copy_to = f\"./best_model_fold{fold_id}.pth\"\n    shutil.copy(best_model_path, copy_to)\n    \n    for p in exp_dir_path.glob(\"*.pth\"):\n        # delete\n        p.unlink()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:06:56.691324Z","iopub.execute_input":"2024-05-10T06:06:56.691608Z","iopub.status.idle":"2024-05-10T06:06:56.949203Z","shell.execute_reply.started":"2024-05-10T06:06:56.691583Z","shell.execute_reply":"2024-05-10T06:06:56.947709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Function for Inference","metadata":{}},{"cell_type":"code","source":"# Function for inference\ndef run_inference_loop(model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[0], device)\n            y = model(x)\n            pred_list.append(y.softmax(dim=1).detach().cpu().numpy())\n    \n    # concatenate to vertical (to df like from long scroll like)\n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:06:56.95057Z","iopub.execute_input":"2024-05-10T06:06:56.950922Z","iopub.status.idle":"2024-05-10T06:06:56.957257Z","shell.execute_reply.started":"2024-05-10T06:06:56.950863Z","shell.execute_reply":"2024-05-10T06:06:56.956388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Inference for test data","metadata":{}},{"cell_type":"code","source":"# Inference for test data\noof_pred_arr = np.zeros((len(train_csv_all_df), CFG.N_CLASSES))\nscore_list = []\n\nfor fold_id in range(CFG.folds):\n    print(f\"\\n[fold {fold_id}]\")\n    device = torch.device(CFG.device)\n\n    # # get_dataloader\n    _, val_path_label, _, val_idx = get_path_label(fold_id, train_csv_all_df)\n    _, val_transform = get_transforms(CFG)\n    val_dataset = Bird2024Dataset(**val_path_label, transform=val_transform)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n    \n    # # get model\n    model_path = f\"./best_model_fold{fold_id}.pth\"\n    model = BirdCLEF2024SpecModel(\n        model_name=CFG.model_name, \n        pretrained=False, \n        num_classes=N_CLASSES, \n        in_channels=1,\n    )\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    \n    # # inference\n    val_pred = run_inference_loop(model, val_loader, device)\n    oof_pred_arr[val_idx] = val_pred\n    \n    del val_idx, val_path_label\n    del model, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:06:56.978996Z","iopub.execute_input":"2024-05-10T06:06:56.979273Z","iopub.status.idle":"2024-05-10T06:07:01.951267Z","shell.execute_reply.started":"2024-05-10T06:06:56.97925Z","shell.execute_reply":"2024-05-10T06:07:01.950264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Calculate CV score","metadata":{}},{"cell_type":"code","source":"# make true array\nLABEL2NUM = make_corresponding_list()\nlabel_arr = train_csv_all_df['primary_label'].apply(lambda x: LABEL2NUM[x]).values\n# one-hot\nture_arr = np.zeros((label_arr.size, N_CLASSES))\nture_arr[np.arange(label_arr.size), label_arr] = 1\nture_arr = pd.DataFrame(ture_arr, columns=CLASSES)\n\n# oof\noof = pd.DataFrame(oof_pred_arr, columns=CLASSES)\n\nmicro_roc_auc_ovr = roc_auc_score(\n    ture_arr,\n    oof,\n    multi_class=\"ovr\",\n    average=\"macro\",\n)\n\nprint(f\"CV: Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.10f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:33:05.100422Z","iopub.execute_input":"2024-05-10T06:33:05.101264Z","iopub.status.idle":"2024-05-10T06:33:05.151922Z","shell.execute_reply.started":"2024-05-10T06:33:05.101227Z","shell.execute_reply":"2024-05-10T06:33:05.150928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(oof.head())\ndisplay(ture_arr.head())\ndisplay(oof.tail())\ndisplay(ture_arr.tail())","metadata":{"execution":{"iopub.status.busy":"2024-05-10T06:35:43.916952Z","iopub.execute_input":"2024-05-10T06:35:43.917705Z","iopub.status.idle":"2024-05-10T06:35:44.681452Z","shell.execute_reply.started":"2024-05-10T06:35:43.91767Z","shell.execute_reply":"2024-05-10T06:35:44.68052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### That's all.","metadata":{}}]}