{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005009,"end_time":"2024-04-13T08:15:08.535649","exception":false,"start_time":"2024-04-13T08:15:08.53064","status":"completed"},"tags":[]},"source":["## Install"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:18.588861Z","iopub.status.busy":"2024-05-28T11:53:18.58814Z","iopub.status.idle":"2024-05-28T11:53:28.159407Z","shell.execute_reply":"2024-05-28T11:53:28.15867Z","shell.execute_reply.started":"2024-05-28T11:53:18.58883Z"},"trusted":true},"outputs":[],"source":["# Basic\n","import sys\n","import os\n","import gc\n","import copy\n","import yaml\n","import bz2\n","import pickle\n","import ast\n","import gzip\n","import random\n","import shutil\n","from time import time\n","import typing as tp\n","import cv2\n","\n","# Python\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import pandas.api.types\n","import scipy\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.model_selection import GroupKFold\n","import sklearn.metrics\n","import pywt\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import librosa\n","import librosa.display\n","\n","# Notebook\n","# from tqdm.notebook import tqdm\n","from tqdm import tqdm\n","from IPython.display import Audio\n","\n","# Pytorch\n","import torch\n","import torchvision\n","from torch import nn\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.cuda import amp\n","from torch.utils.data import Dataset\n","import timm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","\n","# use one device only\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","# delete the display limit of columns\n","pd.set_option('display.max_columns', None)\n","\n","# type elements\n","FilePath = tp.Union[str, Path]\n","Label = tp.Union[int, float, np.ndarray]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Global\n","KAGGLE_TRAIN = '/kaggle/input/birdclef-2024/train_audio'\n","ADDED_TRAIN_1 = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\n","ADDED_TRAIN_2 = '/kaggle/input/birdclef2024-additional-wav-2/additional_audio-2'\n","SAVE_TRAIN = '/kaggle/working/train_image'\n","os.makedirs(KAGGLE_TRAIN, exist_ok=True)\n","\n","make_dataset = False\n","only_over4 = False # sort by data rank"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.003915,"end_time":"2024-04-13T08:15:08.543976","exception":false,"start_time":"2024-04-13T08:15:08.540061","status":"completed"},"tags":[]},"source":["# 0. 目標\n","目標: **鳥の鳴き声の音声ファイルから、鳥の種類を判別/分類すること。**\n","\n","- 提出は184の鳥種を列名に持つcsvとして行うため、各行につき184要素の予測分布を作成する必要がある。  \n","- テストデータには約1100個の4分間の音声データを使用する。\n","- 音声データを区切った、各row_idが示す5秒間に対して予測を行う。\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.003857,"end_time":"2024-04-13T08:15:08.552015","exception":false,"start_time":"2024-04-13T08:15:08.548158","status":"completed"},"tags":[]},"source":["### 0.1 評価指標\n","マクロ平均ROC-AUCスコアを使用します。  \n","[公式Notebook](https://www.kaggle.com/code/metric/birdclef-roc-auc/notebook)による以下のscore関数によって計算されます。\n","\n","・引数  \n","solution: 教師データ  \n","submission: 推論データ  \n","row_id_column_name: 両データに共通する識別id列の列名"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-28T11:53:28.167297Z","iopub.status.busy":"2024-05-28T11:53:28.166976Z","iopub.status.idle":"2024-05-28T11:53:28.178451Z","shell.execute_reply":"2024-05-28T11:53:28.177752Z","shell.execute_reply.started":"2024-05-28T11:53:28.167266Z"},"papermill":{"duration":2.114317,"end_time":"2024-04-13T08:15:10.670372","exception":false,"start_time":"2024-04-13T08:15:08.556055","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# kaggle_metric_utilitiesが使用できないので実際に試すことはできません\n","# import kaggle_metric_utilities\n","\n","class ParticipantVisibleError(Exception):\n","    pass\n","\n","\n","def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n","    '''\n","    Version of macro-averaged ROC-AUC score that ignores all classes that have no true positive labels.\n","    '''\n","    del solution[row_id_column_name]\n","    del submission[row_id_column_name]\n","\n","    if not pandas.api.types.is_numeric_dtype(submission.values):\n","        bad_dtypes = {x: submission[x].dtype  for x in submission.columns if not pandas.api.types.is_numeric_dtype(submission[x])}\n","        raise ParticipantVisibleError(f'Invalid submission data types found: {bad_dtypes}')\n","\n","    solution_sums = solution.sum(axis=0)\n","    scored_columns = list(solution_sums[solution_sums > 0].index.values)\n","    assert len(scored_columns) > 0\n","\n","    return kaggle_metric_utilities.safe_call_score(sklearn.metrics.roc_auc_score, solution[scored_columns].values, submission[scored_columns].values, average='macro')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.004265,"end_time":"2024-04-13T08:15:10.679768","exception":false,"start_time":"2024-04-13T08:15:10.675503","status":"completed"},"tags":[]},"source":["・試しにsample_submissionを提出"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:28.179676Z","iopub.status.busy":"2024-05-28T11:53:28.179422Z","iopub.status.idle":"2024-05-28T11:53:28.235649Z","shell.execute_reply":"2024-05-28T11:53:28.234938Z","shell.execute_reply.started":"2024-05-28T11:53:28.179654Z"},"papermill":{"duration":0.059181,"end_time":"2024-04-13T08:15:10.743448","exception":false,"start_time":"2024-04-13T08:15:10.684267","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["sample_submission = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv') \n","display(sample_submission)\n","sample_submission.to_csv('submission.csv')\n","# Score 0.5"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.004374,"end_time":"2024-04-13T08:15:10.75308","exception":false,"start_time":"2024-04-13T08:15:10.748706","status":"completed"},"tags":[]},"source":["# 1. データ確認\n","与えられたデータは以下の通りです。  \n","・train_audio: 学習用データ    \n","・test_soundscapes: テストデータが格納される  \n","・unlabeled_soundscapes: テストデータと同じ場所で録音された、ラベルのない音声データ   \n","・train_metadata.csv: 学習用データのメタデータ   \n","・sample_submission.csv  \n","・eBird_Taxonomy_v2021.csv: 鳥種間の関係性についてのデータ\n","\n","それぞれについて解説します。\n","- train_audio  \n","ディレクトリ構成は`train_audio/'bird_species_name'/'file_name'.ogg`で、テストデータに合わせて32KHzにダウンサンプリングされた音声データがoog形式で保存されています。  \n","- test_soundscapes  \n","Notebookが提出された時に、このディレクトリに4分間の約1100個の音声データが配置されます。ファイル名はランダムですが、`soundscape_xxxxxx.ogg`という一般的な名前です。  \n","  全データのダウンロードに約5分かかります。  \n","- unlabeled_soundscapes  \n","テストデータと同じ場所で録音された、ラベルのついていない音声データです。  \n","これ以上の説明はありませんでした。事前学習に使えるかもしれません。  \n","- train_metadata.csv  \n","`train_audio`に関するメタデータが記述されています。主要な列の意味は以下の通りです。  \n","・primary_label: 鳥種を識別するコード    \n","例: `amecro`など。`https://ebird.org/species/amecro`のように、URLの末尾に指定するとより詳細な鳥種の情報を確認できます(幾つかのページは壊れています)。  \n","・latitude & longitude: 録音した場所  \n","一部の鳥種には'方言'があるため、地理的な多様性を確保することが推奨されます。  \n","・author: 録音を行ったユーザー名  \n","・filename: このメタデータが紐づけられたファイルの名前  \n","- sample_submission.csv  \n","有効な提出ファイルの例。列の意味は以下の通りです  \n","・row_id: 予測に使用するid  \n","row_idは`soundscape_[soundscape_id]_[end_time]`の形式で指定される  \n","・[bird_id]: 鳥種コードです。182個存在し、それぞれの鳥の確率を予測する必要があります。 \n","- eBird_Taxonomy_v2021.csv\n","鳥種間の関係性についてのデータです。\n","\n","<br>  \n","\n","以下のより詳細な確認は次で行います。    \n","・train_metadata<br>\n","・train_audio<br>\n","・eBird_Taxonomy_v2021.csv<br>   \n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.004328,"end_time":"2024-04-13T08:15:10.762062","exception":false,"start_time":"2024-04-13T08:15:10.757734","status":"completed"},"tags":[]},"source":["### 1.1 train_metadata \n","学習用データのメタデータついて、詳細を確認します。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:28.238806Z","iopub.status.busy":"2024-05-28T11:53:28.238506Z","iopub.status.idle":"2024-05-28T11:53:28.408286Z","shell.execute_reply":"2024-05-28T11:53:28.407254Z","shell.execute_reply.started":"2024-05-28T11:53:28.238782Z"},"papermill":{"duration":0.149003,"end_time":"2024-04-13T08:15:10.915718","exception":false,"start_time":"2024-04-13T08:15:10.766715","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# メタデータ\n","metadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\n","display(metadata.head(5))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00453,"end_time":"2024-04-13T08:15:10.925365","exception":false,"start_time":"2024-04-13T08:15:10.920835","status":"completed"},"tags":[]},"source":["各列について解説します。  \n","・primary_label: 鳥種を識別するコード。例: `amecro`など  \n","・secondary_label: ファイルに含まれる、中心的でない鳥種(紛れ込んだ鳥種)  \n","・type: データの種類。[call, song, ]  \n","・latitude: 録音した場所の緯度(南北)  \n","・longitude: 録音した場所の経度(東西)  \n","・scientific_name: 学名  \n","・common_name: 一般名  \n","・author: 録音を行ったユーザー名  \n","・license: ライセンス  \n","・rating: xenocantによる録音品質の評価。 範囲は[1,5]で、0は未評価を示す  \n","・url: xeno-canto.orgのデータurl  \n","・filename: このメタデータが紐づけられたファイルの名前  \n","\n","次は統計データを確認します。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:28.409805Z","iopub.status.busy":"2024-05-28T11:53:28.409499Z","iopub.status.idle":"2024-05-28T11:53:28.522565Z","shell.execute_reply":"2024-05-28T11:53:28.521716Z","shell.execute_reply.started":"2024-05-28T11:53:28.40978Z"},"papermill":{"duration":0.108152,"end_time":"2024-04-13T08:15:11.038458","exception":false,"start_time":"2024-04-13T08:15:10.930306","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# メタデータの統計データ\n","print('\\n'+ '-'*15 +' '+ f'shape' +' '+ '-'*15)\n","print(metadata.shape)\n","print('\\n'+ '-'*15 +' '+ f'columns' +' '+ '-'*15)\n","print(metadata.columns)\n","print('\\n'+ '-'*15 +' '+ f'info' +' '+ '-'*15)\n","display(metadata.info())\n","print('\\n'+ '-'*15 +' '+ f'null' +' '+ '-'*15)\n","display(metadata.isnull().sum())\n","print('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\n","display(metadata.describe())\n","print('\\n'+ '-'*15 +' '+ f'value_counts' +' '+ '-'*15)\n","\n","count_column = ['primary_label', 'type', 'author', 'rating']\n","for column in count_column:\n","    print('\\n')\n","    display(metadata[column].value_counts())"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006186,"end_time":"2024-04-13T08:15:11.052858","exception":false,"start_time":"2024-04-13T08:15:11.046672","status":"completed"},"tags":[]},"source":["複数のtypeが存在するため、この情報を使用する場合は取り扱いに注意する必要があります。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:28.523918Z","iopub.status.busy":"2024-05-28T11:53:28.52363Z","iopub.status.idle":"2024-05-28T11:53:28.786118Z","shell.execute_reply":"2024-05-28T11:53:28.785237Z","shell.execute_reply.started":"2024-05-28T11:53:28.523894Z"},"papermill":{"duration":0.360618,"end_time":"2024-04-13T08:15:11.420323","exception":false,"start_time":"2024-04-13T08:15:11.059705","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["display(metadata['type'].value_counts())\n","\n","unique_types = []\n","for type_list in metadata['type']:\n","    for type_elem in ast.literal_eval(type_list):\n","        if not type_elem in unique_types:\n","            unique_types.append(type_elem)\n","\n","print('\\n unique_types: ',unique_types)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006708,"end_time":"2024-04-13T08:15:11.434918","exception":false,"start_time":"2024-04-13T08:15:11.42821","status":"completed"},"tags":[]},"source":["**また上記の確認から分かるように、各データ数が500で打ち止めとなっているため、正しいデータを取得する必要があります。**  \n","Shiroさんが追加のデータセットを公開してくれています。"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006372,"end_time":"2024-04-13T08:15:11.448076","exception":false,"start_time":"2024-04-13T08:15:11.441704","status":"completed"},"tags":[]},"source":["### 1.2 train_audio"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006452,"end_time":"2024-04-13T08:15:11.461441","exception":false,"start_time":"2024-04-13T08:15:11.454989","status":"completed"},"tags":[]},"source":["##### 1.2.1 check data (include additional data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:28.787883Z","iopub.status.busy":"2024-05-28T11:53:28.787534Z","iopub.status.idle":"2024-05-28T11:53:40.607954Z","shell.execute_reply":"2024-05-28T11:53:40.606908Z","shell.execute_reply.started":"2024-05-28T11:53:28.78785Z"},"papermill":{"duration":9.775946,"end_time":"2024-04-13T08:15:21.2442","exception":false,"start_time":"2024-04-13T08:15:11.468254","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["metadata_df = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\n","train_path = '/kaggle/input/birdclef-2024/train_audio/'\n","data, rate = librosa.load(train_path + metadata_df.filename[0])\n","print('data shape: ', data.shape)\n","print('sampling rate: ', rate)\n","print(type(data))\n","\n","display(Audio(data[:rate*5], rate=rate)) # choice first channel and first 5s (rate: num of samples per second)\n","display(px.line(y=data[:rate*5], title=metadata_df.common_name[0]))\n","\n","metadata_adwav = pd.read_csv('/kaggle/input/birdclef2024-additional-wav-1/BirdClef2024_additional.csv')\n","train_path_adwav = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\n","data, rate = librosa.load(train_path_adwav + '/asbfly/XC155673.wav')\n","display(Audio(data[:rate*5], rate=rate))\n","display(px.line(y=data[:rate*5]))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.056186,"end_time":"2024-04-13T08:15:21.361009","exception":false,"start_time":"2024-04-13T08:15:21.304823","status":"completed"},"tags":[]},"source":["##### 1.2.2 check MFCC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:40.610104Z","iopub.status.busy":"2024-05-28T11:53:40.609518Z","iopub.status.idle":"2024-05-28T11:53:42.531913Z","shell.execute_reply":"2024-05-28T11:53:42.530979Z","shell.execute_reply.started":"2024-05-28T11:53:40.610072Z"},"papermill":{"duration":12.055665,"end_time":"2024-04-13T08:15:33.470445","exception":false,"start_time":"2024-04-13T08:15:21.41478","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","\n","# Load an audio file\n","y_ogg, sr_ogg = librosa.load('/kaggle/input/birdclef-2024/train_audio/asbfly/XC134896.ogg')\n","y_wav, sr_wav = librosa.load('/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1/asbfly/XC155673.wav')\n","\n","# Extract MFCC\n","mfcc_ogg = librosa.feature.mfcc(y=y_ogg, sr=sr_ogg)\n","mfcc_wav = librosa.feature.mfcc(y=y_wav, sr=sr_wav)\n","mfccs = {'ogg':mfcc_ogg, 'wav':mfcc_wav}\n","\n","# Plot the MFCCs\n","fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n","axes = axes.flatten()\n","\n","for i, mfcc_key in enumerate(mfccs):\n","    ax = axes[i]\n","    print(ax)\n","    print(type(ax))\n","    img = librosa.display.specshow(mfccs[mfcc_key], x_axis='time', ax=ax)\n","    cbar = fig.colorbar(img, ax=ax)\n","    cbar.set_label('Value')\n","    ax.set_title('MFCC_'+f'{mfcc_key}')\n","\n","plt.tight_layout()   \n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.05274,"end_time":"2024-04-13T08:15:33.581056","exception":false,"start_time":"2024-04-13T08:15:33.528316","status":"completed"},"tags":[]},"source":["### 1.3 eBird_Taxonomy_v2021.csv\n","このファイルは分類学的情報を持ちます。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:42.533836Z","iopub.status.busy":"2024-05-28T11:53:42.533225Z","iopub.status.idle":"2024-05-28T11:53:42.686009Z","shell.execute_reply":"2024-05-28T11:53:42.685136Z","shell.execute_reply.started":"2024-05-28T11:53:42.533803Z"},"papermill":{"duration":0.182924,"end_time":"2024-04-13T08:15:33.816548","exception":false,"start_time":"2024-04-13T08:15:33.633624","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["taxonomy = pd.read_csv('/kaggle/input/birdclef-2024/eBird_Taxonomy_v2021.csv')\n","\n","display(taxonomy.head())\n","print('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\n","display(taxonomy.describe())\n","display(taxonomy.describe(include=['O']))\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.059472,"end_time":"2024-04-13T08:15:33.926745","exception":false,"start_time":"2024-04-13T08:15:33.867273","status":"completed"},"tags":[]},"source":["各列について解説します。<br><br>\n","TAXON_ORDER: 鳥種の分類番号   \n","CATEGORY: 鳥種  \n","SPECIES_CODE: 鳥種コード  \n","PRIMARY_COM_NAME: 鳥種の一般名\n","SCI_NAME: 鳥種の学名  \n","ORDER1: 何目か(イヌ目など)  \n","FAMILY: 分類学上の家族  \n","SPECIES_GROUP: 種族グループ  \n","REPORT_AS: 鳥種に関する追加情報  \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Create data and Save"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Load kaggle train metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:42.695002Z","iopub.status.busy":"2024-05-28T11:53:42.69474Z","iopub.status.idle":"2024-05-28T11:53:42.803935Z","shell.execute_reply":"2024-05-28T11:53:42.803011Z","shell.execute_reply.started":"2024-05-28T11:53:42.694979Z"},"trusted":true},"outputs":[],"source":["# Load kaggle train metadata\n","TRAIN_CSV = '/kaggle/input/birdclef-2024/train_metadata.csv'\n","train_csv_df = pd.read_csv(TRAIN_CSV)\n","print(train_csv_df.columns)\n","\n","filename_df = train_csv_df['filename']"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Load metadata added"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:42.805391Z","iopub.status.busy":"2024-05-28T11:53:42.805072Z","iopub.status.idle":"2024-05-28T11:53:43.311134Z","shell.execute_reply":"2024-05-28T11:53:43.310191Z","shell.execute_reply.started":"2024-05-28T11:53:42.805332Z"},"trusted":true},"outputs":[],"source":["# Load metadata added\n","TRAIN_CSV_ADD = '/kaggle/input/birdclef2024-additional-wav-1/BirdClef2024_additional.csv'\n","train_csv_add_df = pd.read_csv(TRAIN_CSV_ADD)\n","\n","print(train_csv_add_df.columns)\n","display(train_csv_add_df)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3 Aligne added metadata "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:43.318084Z","iopub.status.busy":"2024-05-28T11:53:43.317769Z","iopub.status.idle":"2024-05-28T11:53:43.409662Z","shell.execute_reply":"2024-05-28T11:53:43.408815Z","shell.execute_reply.started":"2024-05-28T11:53:43.318057Z"},"trusted":true},"outputs":[],"source":["# Aligne added metadata \n","pd.set_option('future.no_silent_downcasting', True)\n","def Aligne_metadata_added(df:pd.DataFrame):\n","    if 'lat' in df.columns:\n","        # Adjust kaggle_train and additional_train for merge, columns required the processing: file, q , type\n","        alignment_dict = {'latitude':'lat', 'longitude':'lng', 'filename': 'file', 'rating':'q'}\n","        in_alingment_dict = {value: key for key, value in alignment_dict.items()}\n","\n","        # rename\n","        df.rename(columns=in_alingment_dict, inplace=True)\n","        # modify filename\n","        df['filename'] = df['primary_label'] + '/' + df['filename'] + '.wav'\n","        # modify q\n","        dict_q_to_rating = {'A':5, 'B':4, 'C':3, 'D':2, 'E':1, 'no score':0}\n","        df['rating'] = df['rating'].replace(dict_q_to_rating)\n","        df['rating'] = df['rating'].astype('int64')\n","        # modify type\n","        df['type'] = \"[\\'\" + df['type'].str.replace(\", \", \"\\', \\'\") + \"\\']\"\n","        df['added'] = True\n","\n","    return df\n","\n","# check df aligned\n","train_csv_add_df_aligned = Aligne_metadata_added(train_csv_add_df)\n","train_csv_add_df_aligned.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.4 Concat kaggle metadata and added matadata with delete not exist data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:43.424792Z","iopub.status.busy":"2024-05-28T11:53:43.424427Z","iopub.status.idle":"2024-05-28T11:53:44.0097Z","shell.execute_reply":"2024-05-28T11:53:44.008674Z","shell.execute_reply.started":"2024-05-28T11:53:43.424764Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","# check shape\n","print('before: ')\n","print(train_csv_df.shape)\n","print(train_csv_add_df_aligned.shape)\n","\n","# concat\n","train_csv_all_df = pd.concat([train_csv_df, train_csv_add_df_aligned], axis=0).reset_index(drop=True)\n","# add column of stem (ex. XC134896)\n","train_csv_all_df['stem'] = train_csv_all_df['filename'].apply(lambda x: Path(x).stem)\n","\n","# remove not exist data(because because of the link being dead on the website)\n","not_exist_list = [\n","    'aspfly1/XC775312',\n","    'comior1/XC881009',\n","    'hoopoe/XC891005',\n","    'hoopoe/XC891004',\n","    'hoopoe/XC798809',\n","    'hoopoe/XC798808',\n","    'hoopoe/XC798807',\n","    'hoopoe/XC798806',\n","    'hoopoe/XC798805',\n","    'eaywag1/XC835367',\n","    'orihob2/XC762524',\n","]\n","for i, filename in enumerate(not_exist_list):\n","    not_exist_list[i] = filename + '.wav'\n","\n","# Filter the DataFrame\n","train_csv_all_df = train_csv_all_df[~train_csv_all_df['filename'].isin(not_exist_list)]\n","\n","# rating sort\n","if only_over4:\n","    label_counts = train_csv_all_df['primary_label'].value_counts()\n","    CLASSES_OVER10 = label_counts[label_counts > 10].index.tolist()\n","    threshold = 4\n","    train_csv_all_df = train_csv_all_df.drop(train_csv_all_df[(train_csv_all_df['primary_label'].isin(CLASSES_OVER10)) & (train_csv_all_df['rating'] < threshold)].index)\n","\n","# reset index\n","train_csv_all_df = train_csv_all_df.reset_index()\n","\n","# check shape and content\n","print('after concat: ')\n","print(train_csv_all_df.shape)\n","train_csv_all_df"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5 Check statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:44.012096Z","iopub.status.busy":"2024-05-28T11:53:44.011293Z","iopub.status.idle":"2024-05-28T11:53:44.667811Z","shell.execute_reply":"2024-05-28T11:53:44.6669Z","shell.execute_reply.started":"2024-05-28T11:53:44.012055Z"},"trusted":true},"outputs":[],"source":["# statistics data of metadata\n","print('\\n'+ '-'*15 +' '+ f'shape' +' '+ '-'*15)\n","print(train_csv_all_df.shape)\n","print('\\n'+ '-'*15 +' '+ f'columns' +' '+ '-'*15)\n","print(train_csv_all_df.columns)\n","print('\\n'+ '-'*15 +' '+ f'info' +' '+ '-'*15)\n","display(train_csv_all_df.info())\n","print('\\n'+ '-'*15 +' '+ f'null' +' '+ '-'*15)\n","display(train_csv_all_df.isnull().sum())\n","print('\\n'+ '-'*15 +' '+ f'nunique of stem' +' '+ '-'*15)\n","print(train_csv_all_df['stem'].nunique())\n","print('\\n'+ '-'*15 +' '+ f'statistics' +' '+ '-'*15)\n","display(train_csv_all_df.describe())\n","print('\\n'+ '-'*15 +' '+ f'value_counts' +' '+ '-'*15)\n","\n","count_column = ['primary_label', 'type', 'author', 'rating']\n","for column in count_column:\n","    print('\\n')\n","    display(train_csv_all_df[column].value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Split Fold"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 split fold"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:45.575148Z","iopub.status.busy":"2024-05-28T11:53:45.574854Z","iopub.status.idle":"2024-05-28T11:53:47.644733Z","shell.execute_reply":"2024-05-28T11:53:47.643952Z","shell.execute_reply.started":"2024-05-28T11:53:45.575121Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","def split_fold(df:pd.DataFrame):\n","    # config\n","    N_FOLDS = 5\n","    RANDAM_SEED = 42\n","    df['fold'] = -1\n","\n","    # object\n","    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\n","\n","    for i, (train_index, test_index) in enumerate(skf.split(df, df['primary_label'])):\n","        df.loc[test_index, 'fold'] = i\n","    \n","    return df\n","        \n","train_csv_all_df = split_fold(train_csv_all_df)\n","\n","display(train_csv_all_df.head())\n","train_csv_all_df.to_csv('train_csv_all.csv', index=False)\n","\n","# check\n","view_data = False\n","if view_data:\n","    with pd.option_context('display.max_rows', 300):\n","        print(train_csv_all_df.groupby('fold')['primary_label'].value_counts().head(300))"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Pre-Processing\n","##### Make Dataset in here.\n","This is needed for using when inference."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:47.698653Z","iopub.status.busy":"2024-05-28T11:53:47.698332Z","iopub.status.idle":"2024-05-28T11:53:47.796666Z","shell.execute_reply":"2024-05-28T11:53:47.795738Z","shell.execute_reply.started":"2024-05-28T11:53:47.698629Z"},"trusted":true},"outputs":[],"source":["if make_dataset:\n","    class preprocessing():\n","        def __init__(self, AUDIO_DIRECTORY, SAVE_DIRECTORY, view=False):\n","            # config\n","            self.AUDIO_DIRECTORY = AUDIO_DIRECTORY\n","            self.SAVE_DIRECTORY = SAVE_DIRECTORY\n","            self.view = view\n","\n","            # make directory\n","            func_names = [method for method in dir(self) if callable(getattr(self, method)) and method.startswith(\"func\")]\n","            print(func_names)\n","            os.makedirs(self.SAVE_DIRECTORY, exist_ok=True)\n","            for func_name in func_names:\n","                func = func_name.split('_')[-1]\n","                os.makedirs(self.SAVE_DIRECTORY + '/' + func, exist_ok=True)\n","\n","        # apply and save\n","        def apply_func(self, function):\n","            train_dict = {}\n","            species_list = os.listdir(self.AUDIO_DIRECTORY)\n","            for species in species_list:\n","                species_path = self.AUDIO_DIRECTORY + '/' + species\n","                audio_file_list = os.listdir(species_path)\n","                for audio_file in audio_file_list:\n","                    audio_filepath = species_path +'/' + audio_file\n","                    # load audio\n","                    self.load_wave(audio_filepath) \n","                    # apply function\n","                    mfcc = function() \n","                    # register to dict\n","                    train_dict[audio_file.split(\".\")[0]] = mfcc \n","\n","            # set function name as filepath name\n","            SAVE_PATH = self.SAVE_DIRECTORY + '/' + function.__name__.split('_')[-1] + f'/train.pickle.gz'\n","            self.save_as_picke_gzip(train_dict, SAVE_PATH)\n","\n","        def save_as_picke_gzip(self, data, filepath):       \n","            with gzip.open(filepath, 'wb') as f:\n","                pickle.dump(data, f)\n","\n","        def load_wave(self, audio_filepath):\n","            # pick up first 5 seconds\n","            self.y, self.sr = librosa.load(audio_filepath, offset=0, duration=5)\n","\n","        def func_waveform(self):        \n","            if self.view:\n","                print('waveform shape: ', self.y.shape)\n","                display(Audio(self.y, rate=self.sr))\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.waveshow(self.y, sr=self.sr)\n","                plt.title('Waveform')\n","                plt.xlabel('Time (s)')\n","                plt.ylabel('Amplitude')\n","                plt.show()\n","            return self.y\n","\n","        def func_spec(self):\n","            spec = librosa.amplitude_to_db(np.abs(librosa.stft(self.y)), ref=np.max)\n","\n","            if self.view:\n","                print('spec shape: ', spec.shape)\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.specshow(spec, sr=self.sr, x_axis='time', y_axis='log')\n","                plt.colorbar(format='%+2.0f dB')\n","                plt.title('Spectrogram')\n","                plt.show()\n","            return spec\n","\n","        def func_melspec(self):\n","            melspec = librosa.feature.melspectrogram(y=self.y, sr=self.sr, n_mels=128)\n","            melspec_DB = librosa.power_to_db(melspec, ref=np.max)\n","\n","            if self.view:\n","                print('melspec shape: ', melspec_DB.shape)\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.specshow(melspec_DB, sr=self.sr, x_axis='time', y_axis='mel')\n","                plt.colorbar(format='%+2.0f dB')\n","                plt.title('Mel Spectrogram')\n","                plt.show()\n","            return melspec_DB\n","\n","        def func_scalogram(self):\n","            scales = pywt.central_frequency('cmor') / np.linspace(1, 100, 100) * self.sr\n","            cwtmatr, freqs = pywt.cwt(self.y, scales, 'cmor', sampling_period=1/self.sr)\n","\n","            if self.view:\n","                print('scarogram shape: ', cwtmatr.shape)\n","                plt.figure(figsize=(10, 4))\n","                plt.imshow(abs(cwtmatr), aspect='auto', extent=[0, len(self.y) / self.sr, 1, 100], cmap='jet', origin='lower')\n","                plt.colorbar()\n","                plt.title('Scalogram')\n","                plt.xlabel('Time (s)')\n","                plt.ylabel('Scale')\n","                plt.show()\n","            # to real value\n","            return abs(cwtmatr)\n","\n","        def func_chromagram(self):\n","            C = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)\n","\n","            if self.view:\n","                print('chromagram shape: ', C.shape)\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.specshow(C, sr=self.sr, x_axis='time', y_axis='chroma', cmap='coolwarm')\n","                plt.colorbar()\n","                plt.title('Chromagram')\n","                plt.show()\n","            return C\n","\n","\n","        def func_mfcc(self): \n","            mfcc = librosa.feature.mfcc(y=self.y, sr=self.sr)\n","\n","            if self.view:\n","                print('mfcc shape: ', mfcc.shape)\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.specshow(mfcc, sr=self.sr, x_axis='time')\n","                plt.ylabel('MFCC coeffs')\n","                plt.colorbar()\n","                plt.title('MFCC')\n","                plt.show()\n","            return mfcc\n","\n","        def func_spectralcontrast(self):\n","            contrast = librosa.feature.spectral_contrast(y=self.y, sr=self.sr)\n","\n","            if self.view:\n","                print('contrast shape: ', contrast.shape)\n","                plt.figure(figsize=(10, 4))\n","                librosa.display.specshow(contrast, x_axis='time')\n","                plt.colorbar()\n","                plt.ylabel('Frequency bands')\n","                plt.title('Spectral Contrast')\n","                plt.show()\n","            return contrast\n","\n","\n","        def execute(self):\n","            func_list = [\n","    #             self.func_waveform,\n","    #             self.func_spec,\n","    #             self.func_melspec,\n","                self.func_scalogram,\n","    #             self.func_chromagram,\n","    #             self.func_mfcc,\n","    #             self.func_spectralcontrast,\n","            ]\n","            for func in func_list:\n","                self.apply_func(func)\n","\n","    # ・Define preprocessing class\n","    preprocessing_kaggle = preprocessing(KAGGLE_TRAIN, SAVE_TRAIN)\n","    preprocessing_added_train_1 = preprocessing(ADDED_TRAIN_1, SAVE_TRAIN)\n","    preprocessing_added_train_2 = preprocessing(ADDED_TRAIN_2, SAVE_TRAIN)\n","\n","    # ・Execute preprocessing\n","    preprocessing_kaggle.execute()\n","    preprocessing_added_train_1.execute()\n","    preprocessing_added_train_2.execute()\n","\n","\n","\n","    print(os.listdir(SAVE_TRAIN))\n"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Create Dataset"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1 Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:47.817315Z","iopub.status.busy":"2024-05-28T11:53:47.817061Z","iopub.status.idle":"2024-05-28T11:53:47.826096Z","shell.execute_reply":"2024-05-28T11:53:47.825238Z","shell.execute_reply.started":"2024-05-28T11:53:47.817293Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","#     model_name = \"efficientnetv2_rw_s.ra2_in1k\"  # input size: 288\n","    model_name = \"efficientnet_b0.ra_in1k\"   # input size: 224\n","#     model_name = \"efficientvit_b0.r224_in1k\"   # input size: 224\n","\n","    img_size = 224                  # input size \n","    folds = 5                       # number of folds\n","    interpolation = cv2.INTER_AREA  # specifying method of interpolation(dfault is cv2.INTER_LINEAR)\n","    max_epoch = 9                   # number of max epoch. 1epoch means going around the training dataset.\n","    batch_size = 32                 # batch size. Number of samples passed to the network in one training step\n","    lr = 1.0e-03                    # learning rate. determine step size when updating model's weight\n","    weight_decay = 1.0e-02          # weight decay. Append regularization term for prevent over fitting\n","    es_patience = 5                 # epoch number of Early Stopping\n","    seed = 1086                     # seed\n","    deterministic = True            # deterministic\n","    enable_amp = False              # Automatic Mixed Precision\n","    device = \"cuda\"                 # devide for model, \"cuda\" is NVIDIA GPU\n","    simple_training = False         # only use a few data for training.\n","    n_simple = 100                  # number of data of simple_training\n","    ten_tweny_sec = False           # using 10 or 20 sec data instead of 5 sec"]},{"cell_type":"markdown","metadata":{},"source":["##### 5.1.1 Install"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:47.873569Z","iopub.status.busy":"2024-05-28T11:53:47.873295Z","iopub.status.idle":"2024-05-28T11:53:47.89093Z","shell.execute_reply":"2024-05-28T11:53:47.890069Z","shell.execute_reply.started":"2024-05-28T11:53:47.873547Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","sample_submission = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv')\n","TRAIN_IMAGE = Path('/kaggle/input/bird2024-melspec-v6/train_image/melspec')\n","CLASSES = sample_submission.columns[1:].values\n","N_CLASSES = len(CLASSES)\n","if CFG.simple_training:\n","    train_csv_all_df = train_csv_all_df[:CFG.n_simple]\n","    \n","\n","\n","print(CLASSES)\n","print(N_CLASSES)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2 Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:47.900559Z","iopub.status.busy":"2024-05-28T11:53:47.900234Z","iopub.status.idle":"2024-05-28T11:53:47.917629Z","shell.execute_reply":"2024-05-28T11:53:47.916729Z","shell.execute_reply.started":"2024-05-28T11:53:47.900525Z"},"trusted":true},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","\n","# for dataset\n","def get_path_label(val_fold, train_csv_all_df: pd.DataFrame):\n","    \"\"\"\n","    Get file path and label.\n","    \"\"\"\n","    \n","    train_idx = train_csv_all_df[train_csv_all_df[\"fold\"] != val_fold].index.values\n","    val_idx   = train_csv_all_df[train_csv_all_df[\"fold\"] == val_fold].index.values\n","    img_paths = []\n","    labels = train_csv_all_df['primary_label'].values\n","    \n","    for filename in train_csv_all_df[\"filename\"].values:\n","        img_path = TRAIN_IMAGE / filename.replace(\".ogg\", \".npy\").replace(\".wav\", \".npy\")\n","        \n","        if CFG.ten_tweny_sec:\n","            img_path = (TRAIN_IMAGE / (filename + '_10').replace(\".ogg\", \".npy\").replace(\".wav\", \".npy\"))\n","            \n","            if img_path.exists():\n","                img_paths.append(img_path)\n","                \n","        else:\n","            img_paths.append(img_path)\n","\n","    train_data = {\n","        \"image_paths\": [img_paths[idx] for idx in train_idx],\n","        \"labels\": [labels[idx] for idx in train_idx]}\n","\n","    val_data = {\n","        \"image_paths\": [img_paths[idx] for idx in val_idx],\n","        \"labels\": [labels[idx] for idx in val_idx]}\n","    \n","    return train_data, val_data, train_idx, val_idx\n","\n","# make correspondence list of label and number\n","def make_corresponding_list():\n","    \"\"\"\n","    return: list of train_path, list of label_path\n","    \"\"\"\n","    # Correspondences:\n","    # ID -> XC000000, ...\n","    # LABEL -> asbfly, ashdro1, ...\n","    # NUMBER -> 0,1,2, ...\n","\n","\n","    LABEL2NUM = {}\n","    for i, _class in enumerate(CLASSES):\n","        LABEL2NUM[f'{_class}'] = i\n","        NUM2LABEL = {value: key for key, value in LABEL2NUM.items()}\n","\n","    train_path_list = []\n","    ID_list = []\n","    \n","    for species in TRAIN_IMAGE.iterdir():\n","        # iterate train_image dir\n","        for image in species.iterdir():\n","            # iterate image file\n","            if not CFG.simple_training: \n","                # when simple training is false\n","                train_path_list.append(str(image.absolute()))\n","                ID_list.append(str(image.stem))\n","                \n","            else: \n","                # when simple training is ture\n","                if image.stem in train_csv_all_df['stem']:\n","                    train_path_list.append(str(image.absolute()))\n","                    ID_list.append(str(image.stem))                \n","        \n","    ID2LABEL = pd.Series(train_csv_all_df.primary_label.values, index=train_csv_all_df.stem).to_dict()\n","\n","    return LABEL2NUM, NUM2LABEL\n","\n","# set seed\n","def set_random_seed(seed: int = 42, deterministic: bool = False):\n","    \"\"\"Set seeds\"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n","\n","# set tensor to device\n","def to_device(tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]], device: torch.device, *args, **kwargs):\n","    if isinstance(tensors, tuple):\n","        return (t.to(device, *args, **kwargs) for t in tensors)\n","    elif isinstance(tensors, dict):\n","        return {\n","            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n","    else:\n","        return tensors.to(device, *args, **kwargs)\n","\n","\n","# global\n","LABEL2NUM, NUM2LABEL = make_corresponding_list()"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3 Augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_transforms_mixup(CFG):\n","    train_transform = A.Compose([\n","        A.CoarseDropout(max_holes=1, max_height=CFG.img_size, max_width=int(CFG.img_size/5), min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, p=0.5) ,\n","        A.CoarseDropout(max_holes=1, max_height=int(CFG.img_size/7), max_width=CFG.img_size, min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, p=0.5) ,\n","        ToTensorV2(p=1.0)\n","    ])\n","    val_transform = A.Compose([\n","        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n","        ToTensorV2(p=1.0)\n","    ])\n","    return train_transform, val_transform\n","\n","    \n","def get_transforms(CFG):\n","    train_transform = A.Compose([\n","        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n","        # ToTensorV2(p=1.0)\n","    ])\n","    val_transform = A.Compose([\n","        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n","        # ToTensorV2(p=1.0)\n","    ])\n","    return train_transform, val_transform"]},{"cell_type":"markdown","metadata":{},"source":["### 5.4 Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Bird2024Dataset(Dataset):\n","    def __init__(\n","        self,\n","        image_paths: tp.Sequence[FilePath],\n","        labels: tp.Sequence[Label],\n","        transform: A.Compose,\n","    ):\n","        self.train_path_list = image_paths\n","        self.label_list = labels\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        # return total num of data\n","        return len(self.train_path_list)\n","    \n","    def __getitem__(self, index:int):\n","        # return data and target assosiated with index\n","        X = np.load(self.train_path_list[index])\n","        X = self._apply_transform(X)\n","        y = self.label_list[index]\n","        y = LABEL2NUM[y]\n","        \n","        # Number of unique categories\n","        one_hot_y = np.zeros(N_CLASSES)\n","        one_hot_y[y] = 1\n","        #         display(type(X), type(y))\n","\n","        return (X, one_hot_y)\n","    \n","    def _apply_transform(self, img:np.ndarray):\n","        \"\"\"apply transform to image\"\"\"\n","        transformed = self.transform(image=img)\n","        img = transformed[\"image\"]# .float()# .half()\n","        return img\n"]},{"cell_type":"markdown","metadata":{},"source":["### 5.5 Dataset with Mixup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:47.93379Z","iopub.status.busy":"2024-05-28T11:53:47.933434Z","iopub.status.idle":"2024-05-28T11:53:56.546944Z","shell.execute_reply":"2024-05-28T11:53:56.545949Z","shell.execute_reply.started":"2024-05-28T11:53:47.933763Z"},"trusted":true},"outputs":[],"source":["def mixup(image1, image2, label1, label2, alpha=0.5):\n","    \n","    possible_values = np.arange(0, 1.1, 0.1) \n","    p = 0.5\n","    mixed_image = (p * image1) + ((1 - p) * image2)\n","    mixed_label = (p * label1) + ((1 - p) * label2)\n","    \n","    mixed_image = mixed_image.astype(np.single)\n","    # Normalize 0 to min\n","    mixed_image = mixed_image - mixed_image.min()\n","    # Normalize 0 to 255\n","    mixed_image = (mixed_image / mixed_image.max() * 255).astype(np.uint8)\n","    \n","    return mixed_image, mixed_label\n","\n","class MixupDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, alpha=0.5, transform=None):\n","        self.dataset = dataset\n","        self.alpha = alpha\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image1, label1 = self.dataset[idx]\n","        random_number = np.random.rand()\n","        \n","        # mixup\n","        if random_number < self.alpha:\n","            image2, label2 = self.dataset[np.random.randint(0, len(self.dataset))]\n","            mixed_image, mixed_label = mixup(image1, image2, label1, label2, self.alpha)\n","            if self.transform:\n","                mixed_image = self.transform(image=mixed_image)[\"image\"]\n","                mixed_image = mixed_image.float()\n","            return (mixed_image, mixed_label)\n","        # as is\n","        else:\n","            if self.transform:\n","                image1 = self.transform(image=image1)['image']\n","                image1 = image1.float()\n","            return (image1, label1)"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:56.571657Z","iopub.status.busy":"2024-05-28T11:53:56.571418Z","iopub.status.idle":"2024-05-28T11:53:57.554648Z","shell.execute_reply":"2024-05-28T11:53:57.553759Z","shell.execute_reply.started":"2024-05-28T11:53:56.571635Z"},"trusted":true},"outputs":[],"source":["import timm\n","import torch\n","from torch import nn\n","\n","class BirdCLEF2024SpecModel(nn.Module):\n","\n","    def __init__(\n","            self,\n","            model_name: str,\n","            pretrained: bool,\n","            in_channels: int,\n","            num_classes: int,\n","        ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name=model_name, \n","            pretrained=pretrained,\n","            num_classes=num_classes, \n","            in_chans=in_channels\n","        )\n","\n","    def forward(self, x):\n","        h = self.model(x)      \n","\n","        return h\n","\n","model = BirdCLEF2024SpecModel(model_name=CFG.model_name, pretrained=True, num_classes=N_CLASSES, in_channels=1)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:57.556207Z","iopub.status.busy":"2024-05-28T11:53:57.555906Z","iopub.status.idle":"2024-05-28T11:53:58.437314Z","shell.execute_reply":"2024-05-28T11:53:58.436381Z","shell.execute_reply.started":"2024-05-28T11:53:57.556179Z"},"trusted":true},"outputs":[],"source":["def show_batch(ds, row=3, col=3):\n","    fig = plt.figure(figsize=(10, 10))\n","    img_index = np.random.randint(0, len(ds)-1, row*col)\n","    \n","    for i in range(len(img_index)):\n","        img, label = ds[img_index[i]]\n","        \n","        if isinstance(img, torch.Tensor):\n","            img = img.detach().numpy()\n","            img = np.squeeze(img)\n","        \n","        ax = fig.add_subplot(row, col, i + 1, xticks=[], yticks=[])\n","        ax.imshow(img, cmap='jet')\n","        ax.set_title(f'ID: {img_index[i]}; Target: {label}')\n","    \n","    plt.tight_layout()\n","    plt.show()\n","    \n","\n","\n","\n","if True:\n","    _train_path_label, _val_path_label, _, _ = get_path_label(0, train_csv_all_df)\n","    _train_transform, _val_transform = get_transforms(CFG)\n","    _train_transform_mixup, _val_transform_mixup = get_transforms_mixup(CFG)\n","\n","    # Normal Dataset\n","    # _train_dataset = Bird2024Dataset(**_train_path_label, transform=_train_transform)\n","    # Mixup Dataset\n","    _train_dataset = MixupDataset(Bird2024Dataset(**_train_path_label, transform=_train_transform), transform=_train_transform_mixup)    \n","    test_input, test_target = _train_dataset[0]\n","    \n","    show_batch(_train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:58.438803Z","iopub.status.busy":"2024-05-28T11:53:58.438523Z","iopub.status.idle":"2024-05-28T11:53:58.446987Z","shell.execute_reply":"2024-05-28T11:53:58.446105Z","shell.execute_reply.started":"2024-05-28T11:53:58.438779Z"},"trusted":true},"outputs":[],"source":["class FocalLossBCE(torch.nn.Module):\n","    def __init__(\n","        self,\n","        alpha: float = 0.25,\n","        gamma: float = 2,\n","        reduction: str = \"mean\",\n","        bce_weight: float = 1.0,\n","        focal_weight: float = 1.0,\n","    ):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n","        self.bce_weight = bce_weight\n","        self.focal_weight = focal_weight\n","\n","    def forward(self, inputs, targets):\n","        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n","            inputs=inputs,\n","            targets=targets,\n","            alpha=self.alpha,\n","            gamma=self.gamma,\n","            reduction=self.reduction,\n","        )\n","        bce_loss = self.bce(inputs, targets)\n","        return self.bce_weight * bce_loss + self.focal_weight * focall_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:58.448705Z","iopub.status.busy":"2024-05-28T11:53:58.448331Z","iopub.status.idle":"2024-05-28T11:53:58.469659Z","shell.execute_reply":"2024-05-28T11:53:58.468638Z","shell.execute_reply.started":"2024-05-28T11:53:58.448675Z"},"trusted":true},"outputs":[],"source":["def train_one_fold(CFG, val_fold, train_all, output_path):\n","    \"\"\"Main\"\"\"\n","    # If True, forces cuDNN to benchmark multiple convolution algorithms and choose the fastest one\n","    torch.backends.cudnn.benchmark = True\n","    set_random_seed(CFG.seed, deterministic=CFG.deterministic)\n","    # set device with pytorch env\n","    device = torch.device(CFG.device)\n","    \n","    train_path_label, val_path_label, _, _ = get_path_label(val_fold, train_csv_all_df)\n","    train_transform, val_transform = get_transforms(CFG)\n","    train_transform_mixup, val_transform_mixup = get_transforms_mixup(CFG)\n","    \n","#     train_dataset = Bird2024Dataset(**train_path_label, transform=train_transform)\n","#     val_dataset = Bird2024Dataset(**val_path_label, transform=val_transform)\n","    train_dataset = MixupDataset(Bird2024Dataset(**_train_path_label, transform=_train_transform), transform=train_transform_mixup)\n","    val_dataset = MixupDataset(Bird2024Dataset(**val_path_label, transform=val_transform), transform=val_transform_mixup)\n","    \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=True, drop_last=True)\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n","    \n","    model = BirdCLEF2024SpecModel(\n","        model_name=CFG.model_name, \n","        pretrained=True, \n","        num_classes=N_CLASSES, \n","        in_channels=1\n","    )\n","    model.to(device)\n","    \n","    optimizer = optim.AdamW(params=model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n","    scheduler = lr_scheduler.OneCycleLR(\n","        optimizer=optimizer, epochs=CFG.max_epoch,\n","        pct_start=0.0, steps_per_epoch=len(train_loader),\n","        max_lr=CFG.lr, div_factor=25, final_div_factor=4.0e-01\n","    )\n","    \n","# type of loss function(train)\n","#     loss_func = KLDivLossWithLogits()\n","#     loss_func = nn.CrossEntropyLoss()\n","    loss_func = nn.BCEWithLogitsLoss()\n","#     loss_func = FocalLossBCE()\n","    loss_func.to(device)\n","    \n","# type of loss function(test)\n","#     loss_func_val = KLDivLossWithLogitsForVal()\n","#     loss_func_val = nn.CrossEntropyLoss()\n","    loss_func_val = nn.BCEWithLogitsLoss()\n","#     loss_func_val = FocalLossBCE()\n","    \n","    use_amp = CFG.enable_amp\n","    scaler = amp.GradScaler(enabled=use_amp)\n","    \n","    best_val_loss = 1.0e+09\n","    best_epoch = 0\n","    train_loss = 0\n","    val_loss = 0\n","    \n","    for epoch in range(1, CFG.max_epoch + 1):\n","        epoch_start = time()\n","        model.train()\n","        for batch in train_loader:\n","            \n","            x, t = batch\n","            x = to_device(x, device)\n","            t = to_device(t, device)\n","                \n","            optimizer.zero_grad()\n","            with amp.autocast(use_amp):\n","                y = model(x)\n","#                 y = torch.sigmoid(y) # as necessary\n","                loss = loss_func(y, t)\n","            \n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            train_loss += loss.item()\n","            scheduler.step()\n","            \n","        train_loss /= len(train_loader)\n","            \n","        model.eval()\n","        for batch in val_loader:\n","            x, t = batch\n","            x = to_device(x, device)\n","            with torch.no_grad(), amp.autocast(use_amp):\n","                y = model(x)\n","#                 y = torch.sigmoid(y) # as necessary\n","            y = y.detach().cpu().to(torch.float32)\n","            loss = loss_func_val(y, t)\n","            val_loss += loss.item()\n","        val_loss /= len(val_loader)\n","        \n","        if val_loss < best_val_loss:\n","            best_epoch = epoch\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), str(output_path / f'snapshot_epoch_{epoch}.pth'))\n","        \n","        elapsed_time = time() - epoch_start\n","        print(\n","            f\"[epoch {epoch}] train loss: {train_loss: .6f}, val loss: {val_loss: .6f}, elapsed_time: {elapsed_time: .3f}\")\n","        \n","        if epoch - best_epoch > CFG.es_patience:\n","            print(\"Early Stopping!\")\n","            break\n","            \n","        train_loss = 0\n","        val_loss = 0\n","            \n","    return val_fold, best_epoch, best_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:53:58.471029Z","iopub.status.busy":"2024-05-28T11:53:58.470767Z","iopub.status.idle":"2024-05-28T11:54:49.384506Z","shell.execute_reply":"2024-05-28T11:54:49.383147Z","shell.execute_reply.started":"2024-05-28T11:53:58.471006Z"},"trusted":true},"outputs":[],"source":["score_list = []\n","for fold_id in range(CFG.folds):\n","    output_path = Path(f\"fold{fold_id}\")\n","    output_path.mkdir(exist_ok=True)\n","    print(f\"[fold{fold_id}]\")\n","    score_list.append(train_one_fold(CFG, fold_id, train_csv_all_df, output_path))"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:49.386998Z","iopub.status.busy":"2024-05-28T11:54:49.38659Z","iopub.status.idle":"2024-05-28T11:54:49.392538Z","shell.execute_reply":"2024-05-28T11:54:49.391699Z","shell.execute_reply.started":"2024-05-28T11:54:49.386943Z"},"trusted":true},"outputs":[],"source":["# check the best score and fold\n","print(score_list)"]},{"cell_type":"markdown","metadata":{},"source":["##### select the best model and delete others"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:49.394442Z","iopub.status.busy":"2024-05-28T11:54:49.393618Z","iopub.status.idle":"2024-05-28T11:54:49.580139Z","shell.execute_reply":"2024-05-28T11:54:49.579173Z","shell.execute_reply.started":"2024-05-28T11:54:49.39441Z"},"trusted":true},"outputs":[],"source":["# select the best model and delete others\n","best_log_list = []\n","for (fold_id, best_epoch, _) in score_list:\n","    \n","    # select the best model\n","    exp_dir_path = Path(f\"fold{fold_id}\")\n","    best_model_path = exp_dir_path / f\"snapshot_epoch_{best_epoch}.pth\"\n","    # copy to new place\n","    copy_to = f\"./best_model_fold{fold_id}.pth\"\n","    shutil.copy(best_model_path, copy_to)\n","    \n","    for p in exp_dir_path.glob(\"*.pth\"):\n","        # delete\n","        p.unlink()"]},{"cell_type":"markdown","metadata":{},"source":["##### Function for Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:49.588788Z","iopub.status.busy":"2024-05-28T11:54:49.588494Z","iopub.status.idle":"2024-05-28T11:54:49.594833Z","shell.execute_reply":"2024-05-28T11:54:49.594025Z","shell.execute_reply.started":"2024-05-28T11:54:49.588763Z"},"trusted":true},"outputs":[],"source":["# Function for inference\n","def run_inference_loop(model, loader, device):\n","    model.to(device)\n","    model.eval()\n","    pred_list = []\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            x = to_device(batch[0], device)\n","            y = model(x)\n","#             y = torch.sigmoid(x) # as necessary\n","            pred_list.append(y.softmax(dim=1).detach().cpu().numpy())\n","    \n","    # concatenate in vertical (long scroll like)\n","    pred_arr = np.concatenate(pred_list)\n","    del pred_list\n","    return pred_arr"]},{"cell_type":"markdown","metadata":{},"source":["##### Inference for test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:49.610436Z","iopub.status.busy":"2024-05-28T11:54:49.610118Z","iopub.status.idle":"2024-05-28T11:54:53.488821Z","shell.execute_reply":"2024-05-28T11:54:53.487794Z","shell.execute_reply.started":"2024-05-28T11:54:49.610408Z"},"trusted":true},"outputs":[],"source":["# Inference for test data\n","after_runned = True\n","if after_runned:\n","\n","    oof_pred_arr = np.zeros((len(train_csv_all_df), N_CLASSES))\n","    score_list = []\n","\n","    for fold_id in range(CFG.folds):\n","        print(f\"\\n[fold {fold_id}]\")\n","        device = torch.device(CFG.device)\n","\n","        # # get_dataloader\n","        _, val_path_label, _, val_idx = get_path_label(fold_id, train_csv_all_df)\n","        _, val_transform = get_transforms(CFG)\n","        _, val_transform_mixup = get_transforms_mixup(CFG)\n","        \n","        val_dataset = MixupDataset(Bird2024Dataset(**val_path_label, transform=val_transform), transform=val_transform_mixup)\n","#         val_dataset = Bird2024Dataset(**val_path_label, transform=val_transform)\n","        val_loader = torch.utils.data.DataLoader(\n","            val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n","\n","        # get saved model\n","        model_path = f\"./best_model_fold{fold_id}.pth\"\n","        model = BirdCLEF2024SpecModel(\n","            model_name=CFG.model_name, \n","            pretrained=False, \n","            num_classes=N_CLASSES, \n","            in_channels=1,\n","        )\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","\n","        # inference\n","        val_pred = run_inference_loop(model, val_loader, device)\n","#         val_pred = scipy.special.softmax(val_pred)\n","        oof_pred_arr[val_idx] = val_pred\n","\n","        del val_idx, val_path_label\n","        del model, val_loader\n","        torch.cuda.empty_cache()\n","        gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate CV score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:53.490998Z","iopub.status.busy":"2024-05-28T11:54:53.490704Z","iopub.status.idle":"2024-05-28T11:54:53.521179Z","shell.execute_reply":"2024-05-28T11:54:53.520054Z","shell.execute_reply.started":"2024-05-28T11:54:53.490973Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","# make true array(teacher label)\n","label_arr = train_csv_all_df['primary_label'].apply(lambda x: LABEL2NUM[x]).values\n","# one-hot\n","ture_arr = np.zeros((label_arr.size, N_CLASSES))\n","ture_arr[np.arange(label_arr.size), label_arr] = 1\n","ture_arr = pd.DataFrame(ture_arr, columns=CLASSES)\n","\n","# oof\n","oof = pd.DataFrame(oof_pred_arr, columns=CLASSES)\n","\n","micro_roc_auc_ovr = roc_auc_score(\n","    ture_arr,\n","    oof,\n","    multi_class=\"ovr\",\n","    average=\"micro\",\n",")\n","\n","print(f\"CV: Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.10f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:53.52305Z","iopub.status.busy":"2024-05-28T11:54:53.522275Z","iopub.status.idle":"2024-05-28T11:54:53.529053Z","shell.execute_reply":"2024-05-28T11:54:53.528173Z","shell.execute_reply.started":"2024-05-28T11:54:53.523024Z"},"trusted":true},"outputs":[],"source":["# check whether the sum of row will be 1\n","display(oof.loc[0].sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T11:54:53.530481Z","iopub.status.busy":"2024-05-28T11:54:53.530096Z","iopub.status.idle":"2024-05-28T11:54:54.412322Z","shell.execute_reply":"2024-05-28T11:54:54.41135Z","shell.execute_reply.started":"2024-05-28T11:54:53.530456Z"},"trusted":true},"outputs":[],"source":["# check the results\n","display(oof.head())\n","display(ture_arr.head())\n","display(oof.tail())\n","display(ture_arr.tail())"]},{"cell_type":"markdown","metadata":{},"source":["#### END"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8068726,"sourceId":70203,"sourceType":"competition"},{"datasetId":4745108,"sourceId":8048860,"sourceType":"datasetVersion"},{"datasetId":4746211,"sourceId":8048895,"sourceType":"datasetVersion"},{"datasetId":4746338,"sourceId":8048897,"sourceType":"datasetVersion"},{"datasetId":4963066,"isSourceIdPinned":true,"sourceId":8353101,"sourceType":"datasetVersion"},{"datasetId":4963056,"sourceId":8446576,"sourceType":"datasetVersion"},{"datasetId":4963063,"sourceId":8449682,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":31.037946,"end_time":"2024-04-13T08:15:36.967243","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-13T08:15:05.929297","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
