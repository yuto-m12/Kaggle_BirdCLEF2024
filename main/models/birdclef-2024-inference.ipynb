{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005009,"end_time":"2024-04-13T08:15:08.535649","exception":false,"start_time":"2024-04-13T08:15:08.53064","status":"completed"},"tags":[]},"source":["# First\n","\n","This is an inference notebook.<br>\n","The training and dataset notebooks are existing."]},{"cell_type":"markdown","metadata":{},"source":["#### Import "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T10:11:08.631038Z","iopub.status.busy":"2024-05-20T10:11:08.630609Z","iopub.status.idle":"2024-05-20T10:13:43.893667Z","shell.execute_reply":"2024-05-20T10:13:43.892076Z","shell.execute_reply.started":"2024-05-20T10:11:08.631002Z"},"trusted":true},"outputs":[],"source":["from time import time\n","t1 = time()\n","# onnxsim-0.4.36\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxsim-0.4.36/onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","\n","# onnxruntime-1.17.3\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/humanfriendly-10.0-py2.py3-none-any.whl\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/coloredlogs-15.0.1-py2.py3-none-any.whl\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n","\n","# onnxconverter-common-1.14.0\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxconverter-common-1.14.0/protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxconverter-common-1.14.0/onnxconverter_common-1.14.0-py2.py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/fastjsonschema-2.17.1/fastjsonschema-2.17.1-py3-none-any.whl\n","\n","# openvino-dev-2024.0.0\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnx-1.15.0/onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/networkx-3.1-py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino_telemetry-2024.1.0-py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino-2024.0.0-14509-cp310-cp310-manylinux2014_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino_dev-2024.0.0-14509-py3-none-any.whl\n","\n","t2 = time()\n","print('Import time: ', f\"{(t2-t1)/60}m\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-20T10:13:43.897254Z","iopub.status.busy":"2024-05-20T10:13:43.896894Z","iopub.status.idle":"2024-05-20T10:13:45.328728Z","shell.execute_reply":"2024-05-20T10:13:45.322062Z","shell.execute_reply.started":"2024-05-20T10:13:43.897223Z"},"trusted":true},"outputs":[],"source":["# Basic\n","import sys\n","import os\n","import gc\n","import copy\n","import yaml\n","import random\n","import shutil\n","from time import time\n","import gzip\n","import bz2\n","\n","# Python\n","import numpy as np\n","import pandas as pd\n","import pandas.api.types\n","import pickle\n","import pywt\n","import librosa\n","import librosa.display\n","from pathlib import Path\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GroupKFold\n","from sklearn.model_selection import StratifiedGroupKFold\n","import sklearn.metrics\n","import matplotlib.pyplot as plt \n","import plotly.express as px\n","import typing as tp\n","import cv2\n","from scipy.special import softmax\n","from glob import glob\n","\n","# Notebook\n","from IPython.display import Audio\n","from tqdm.notebook import tqdm\n","\n","# PyTorch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.cuda import amp\n","import timm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# For faster inference\n","import onnx\n","# import onnxruntime as rt\n","from onnxconverter_common import float16\n","import openvino\n","import openvino as ov\n","\n","\n","\n","\n","# Use one device only\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","# To show all af column\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","metadata":{},"source":["#### Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.33023Z","iopub.status.idle":"2024-05-20T10:13:45.330923Z","shell.execute_reply":"2024-05-20T10:13:45.330652Z","shell.execute_reply.started":"2024-05-20T10:13:45.33063Z"},"trusted":true},"outputs":[],"source":["# test audio dir\n","TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/test_soundscapes')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.334199Z","iopub.status.idle":"2024-05-20T10:13:45.334943Z","shell.execute_reply":"2024-05-20T10:13:45.334624Z","shell.execute_reply.started":"2024-05-20T10:13:45.334595Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    def __init__(self):\n","        self.TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/test_soundscapes')\n","        self.NO_SOUNDSCAPE = False\n","        if 1 >= len(os.listdir(self.TEST_SOUNDSCAPE)):\n","            self.TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/train_audio/asbfly')\n","            self.NO_SOUNDSCAPE = True\n","\n","    model_name = \"efficientnet_b0.ra_in1k\"  # model  224\n","#     model_name = \"efficientvit_b0.r224_in1k\"  # model 224\n","#     model_name = \"efficientnetv2_rw_s.ra2_in1k\"  # model 288\n","    img_size = 224            # input size.\n","    n_folds = 5\n","    interpolation = cv2.INTER_AREA  # specifying method of interpolation(dfault is cv2.INTER_LINEAR)\n","    max_epoch = 9                   # number of max epoch. 1epoch means going around the training dataset.\n","#     batch_size = 32               # train batch size. Number of samples passed to the network in one training step\n","    batch_size = 1                  # test batch size. openvino can't respond flexibility to leftovers of number of batches \n","    lr = 1.0e-03                    # learning rate. determine step size when updating model's weight\n","    weight_decay = 1.0e-02          # weight decay. Append regularization term for prevent over fitting\n","    es_patience = 5                 # Early Stopping\n","    seed = 1086                     # seed\n","    deterministic = True            # enable deterministic behaviour or not\n","    enable_amp = False              # enable Automatic Mixed Precision or not\n","#     device = \"cuda\"               # Device to use training. \"cuda\" is NVIDIA GPU\n","    device = \"cpu\"                  # Device to use training.\n","    \n","    # test related\n","    simple_training = True      # only use few data with training, be enable in training\n","    simple_inferring = False    # only use few data with inferring\n","    n_simple = 100              # number of data with simple training\n","    test = True                 # when inference\n","    show = False                # show few batch data at the end\n","    \n","    # Data Loading related\n","    MELSPEC_H = 128     # Horizontal melspectrogram resolution\n","    TOP_DB = 100        # Maximum decibel to clip audio to\n","    MIN_RATING = 0.0    # Minimum rating\n","    SR = 32000          # Sample rate as provided in competition description\n","    N_FFT = 2000\n","    HOP_LENGTH = 500\n","    fmin = 20\n","    fmax = 16000\n","    \n","    # faster inference related\n","    INPUT_SHAPE: list[int] = [1, 1, img_size, img_size]\n","    DUMMY_INPUT_TENSOR: torch.Tensor = torch.randn(*INPUT_SHAPE)\n","    DUMMY_INPUT_NUMPY_FP32: np.ndarray = DUMMY_INPUT_TENSOR.numpy()\n","    DUMMY_INPUT_NUMPY_FP16: np.ndarray = DUMMY_INPUT_NUMPY_FP32.astype(np.float16)\n","    OUTPUT_DIR_ONNX: Path = Path('./model/onnx')\n","    OUTPUT_DIR_OV: Path = Path('./model/ov')\n","\n","\n","\n","\n","CFG = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.336444Z","iopub.status.idle":"2024-05-20T10:13:45.337074Z","shell.execute_reply":"2024-05-20T10:13:45.336781Z","shell.execute_reply.started":"2024-05-20T10:13:45.336756Z"},"trusted":true},"outputs":[],"source":["# model output dir\n","CFG.OUTPUT_DIR_ONNX.mkdir(parents=True, exist_ok=True)\n","CFG.OUTPUT_DIR_OV.mkdir(parents=True, exist_ok=True)\n","\n","# type elements\n","FilePath = tp.Union[str, Path]\n","Label = tp.Union[int, float, np.ndarray]"]},{"cell_type":"markdown","metadata":{},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.338849Z","iopub.status.idle":"2024-05-20T10:13:45.339428Z","shell.execute_reply":"2024-05-20T10:13:45.339166Z","shell.execute_reply.started":"2024-05-20T10:13:45.339143Z"},"trusted":true},"outputs":[],"source":["# audio file\n","KAGGLE_TRAIN = '/kaggle/input/birdclef-2024/train_audio'\n","ADDED_TRAIN = '/kaggle/input/birdclef2024-additional-mp3/additional_audio'\n","ADDED_TRAIN_1 = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\n","ADDED_TRAIN_2 = '/kaggle/input/birdclef2024-additional-wav-2/additional_audio-2'\n","TEST_SOUNDSCAPE = CFG.TEST_SOUNDSCAPE\n","os.makedirs(KAGGLE_TRAIN, exist_ok=True)\n","\n","# save image dir, that obtrained from audio\n","SAVE_TRAIN = '/kaggle/working/train_image'\n","SAVE_TEST = '/kaggle/working/test_image'\n","\n","# input image dir\n","TRAIN_IMAGE = Path('/kaggle/input/bird2024-melspec-v6/train_image/melspec')\n","TEST_IMAGE = Path('/kaggle/working/test_image/melspec')\n","\n","# trained model\n","TRAINED_MODEL = Path('/kaggle/input/birdcref-2024-introduction-withtraining-train-v2')\n","\n","class preprocessing():\n","    def __init__(self, AUDIO_DIRECTORY, SAVE_DIRECTORY, view=False, test=CFG.test):\n","        # config\n","        self.AUDIO_DIRECTORY = AUDIO_DIRECTORY\n","        self.SAVE_DIRECTORY = SAVE_DIRECTORY\n","        self.view = view\n","        self.test = test\n","        \n","        # make directory\n","        func_names = [method for method in dir(self) if callable(getattr(self, method)) and method.startswith(\"func\")]\n","        print(func_names)\n","        os.makedirs(self.SAVE_DIRECTORY, exist_ok=True)\n","        for func_name in func_names:\n","            func = func_name.split('_')[-1]\n","            os.makedirs(self.SAVE_DIRECTORY + '/' + func, exist_ok=True)\n","    \n","    # load data\n","    def load_wave(self, audio_filepath, offset=0):\n","        # pick up 5 seconds\n","        if CFG.NO_SOUNDSCAPE:\n","            offset = 0\n","        self.y, _ = librosa.load(audio_filepath, sr=CFG.SR , offset=offset, duration=5)\n","        self.sr = CFG.SR\n","    \n","    def normalize(self, data: np.ndarray):\n","        # exclude upper and lower\n","        # np.clip(img,np.exp(-4), np.exp(8))\n","        \n","        data = data.astype(np.single)                       # to single\n","        data = data - data.min()                            # Normalize 0 to min\n","        data = (data / data.max() * 255).astype(np.uint8)   # Normalize 0 to 255\n","        \n","        return data\n","        \n","    # apply and save\n","    def apply_func(self, function):\n","        species_list = os.listdir(self.AUDIO_DIRECTORY)\n","        if not self.test:\n","            for species in species_list:\n","                species_path = self.AUDIO_DIRECTORY + '/' + species\n","                audio_file_list = os.listdir(species_path)\n","                for audio_file in audio_file_list:\n","                    audio_filepath = species_path + '/' + audio_file\n","                    self.load_wave(audio_filepath) # load audio\n","                    output = function() # apply function\n","                    output = self.normalize(output)\n","                    \n","                    SAVE_DIRECTORY = Path(self.SAVE_DIRECTORY + '/' + function.__name__.split('_')[-1] + '/' + species)\n","                    SAVE_DIRECTORY.mkdir(exist_ok=True)\n","                    SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file.split('.')[0]}.npy\"\n","                    np.save(SAVE_PATH, output)\n","                    del output\n","\n","        if self.test:\n","            audio_directory_path = self.AUDIO_DIRECTORY\n","            audio_length = int(4*60) # second\n","            audio_offset_unit_max = int(audio_length / 5)\n","            for audio_file_path in audio_directory_path.glob('*.ogg'):\n","                    for audio_offset_unit in range(audio_offset_unit_max):\n","                        audio_offset = audio_offset_unit * 5\n","                        self.load_wave(str(audio_file_path), audio_offset) # load audio\n","                        output = function() # apply function\n","                        output = self.normalize(output)\n","\n","                        SAVE_DIRECTORY = Path(self.SAVE_DIRECTORY) / function.__name__.split('_')[-1]\n","                        SAVE_DIRECTORY.mkdir(exist_ok=True)\n","                        if CFG.NO_SOUNDSCAPE:\n","                            SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file_path.stem.replace('XC','')}_{audio_offset+5}.npy\" # [soundscape_id]_[end_time].npy\n","                        else:\n","                            SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file_path.stem.replace('soundscape_','')}_{audio_offset+5}.npy\" # [soundscape_id]_[end_time].npy\n","                        np.save(SAVE_PATH, output)\n","\n","                        del output\n","        \n","    def save_as_picke_gzip(self, data, filepath):       \n","        with gzip.open(filepath, 'wb') as f:\n","            pickle.dump(data, f)\n","            \n","    def func_waveform(self):        \n","        if self.view:\n","            print('waveform shape: ', self.y.shape)\n","            display(Audio(self.y, rate=self.sr))\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.waveshow(self.y, sr=self.sr)\n","            plt.title('Waveform')\n","            plt.xlabel('Time (s)')\n","            plt.ylabel('Amplitude')\n","            plt.show()\n","        return self.y\n","    \n","    def func_spec(self):\n","        spec = librosa.amplitude_to_db(\n","            np.abs(librosa.stft(self.y)), \n","            ref=np.max,\n","        )\n","        \n","        if self.view:\n","            print('spec shape: ', spec.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(spec, sr=self.sr, x_axis='time', y_axis='log')\n","            plt.colorbar(format='%+2.0f dB')\n","            plt.title('Spectrogram')\n","            plt.show()\n","        return spec\n","    \n","    def func_melspec(self):\n","        melspec = librosa.feature.melspectrogram(\n","            y=self.y, \n","            sr=CFG.SR,                  # sample rate\n","            n_fft=CFG.N_FFT,            # number of samples in window \n","            hop_length=CFG.HOP_LENGTH,  # step size of window\n","            n_mels=CFG.MELSPEC_H,       # horizontal resolution from fmin→fmax in log scale\n","            fmin=CFG.fmin,              # minimum frequency\n","            fmax=CFG.fmax,              # maximum frequency\n","            power=2.0,                  # intensity^(power) for log scale\n","        )\n","        melspec = librosa.power_to_db(melspec, ref=np.max)\n","        \n","        if self.view:\n","            print('melspec shape: ', melspec.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(melspec, sr=self.sr, x_axis='time', y_axis='mel')\n","            plt.colorbar(format='%+2.0f dB')\n","            plt.title('Mel Spectrogram')\n","            plt.show()\n","        return melspec\n","    \n","    def func_scalogram(self):\n","        scales = pywt.central_frequency('cmor') / np.linspace(1, 100, 100) * self.sr\n","        cwtmatr, freqs = pywt.cwt(self.y, scales, 'cmor', sampling_period=1/self.sr)\n","        \n","        if self.view:\n","            print('scarogram shape: ', cwtmatr.shape)\n","            plt.figure(figsize=(10, 4))\n","            plt.imshow(abs(cwtmatr), aspect='auto', extent=[0, len(self.y) / self.sr, 1, 100], cmap='jet', origin='lower')\n","            plt.colorbar()\n","            plt.title('Scalogram')\n","            plt.xlabel('Time (s)')\n","            plt.ylabel('Scale')\n","            plt.show()\n","            \n","        # to real value\n","        return abs(cwtmatr)\n","\n","    def func_chromagram(self):\n","        C = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('chromagram shape: ', C.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(C, sr=self.sr, x_axis='time', y_axis='chroma', cmap='coolwarm')\n","            plt.colorbar()\n","            plt.title('Chromagram')\n","            plt.show()\n","        return C\n","\n","    \n","    def func_mfcc(self): \n","        mfcc = librosa.feature.mfcc(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('mfcc shape: ', mfcc.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(mfcc, sr=self.sr, x_axis='time')\n","            plt.ylabel('MFCC coeffs')\n","            plt.colorbar()\n","            plt.title('MFCC')\n","            plt.show()\n","        return mfcc\n","\n","    def func_spectralcontrast(self):\n","        contrast = librosa.feature.spectral_contrast(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('contrast shape: ', contrast.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(contrast, x_axis='time')\n","            plt.colorbar()\n","            plt.ylabel('Frequency bands')\n","            plt.title('Spectral Contrast')\n","            plt.show()\n","        return contrast\n","\n","    \n","    def execute(self):\n","        func_list = [\n","            # self.func_waveform,\n","            # self.func_spec,\n","            self.func_melspec,\n","            # self.func_scalogram,\n","            # self.func_chromagram,\n","            # self.func_mfcc,\n","            # self.func_spectralcontrast,\n","        ]\n","        for func in func_list:\n","            self.apply_func(func)\n","\n","# ・Define preprocessing class\n","# preprocessing_kaggle = preprocessing(KAGGLE_TRAIN, SAVE_TRAIN, view=True)\n","# preprocessing_added_train = preprocessing(ADDED_TRAIN, SAVE_TRAIN)\n","# preprocessing_added_train_1 = preprocessing(ADDED_TRAIN_1, SAVE_TRAIN)\n","# preprocessing_added_train_2 = preprocessing(ADDED_TRAIN_2, SAVE_TRAIN)\n","preprocessing_test = preprocessing(TEST_SOUNDSCAPE, SAVE_TEST, test=True)\n","\n","t1 = time()\n","# ・Execute preprocessing\n","# preprocessing_kaggle.execute()\n","# preprocessing_added_train.execute()\n","# preprocessing_added_train_1.execute()\n","# preprocessing_added_train_2.execute()\n","preprocessing_test.execute()\n","t2 = time()\n","\n","\n","print('Preprocessing time: ', f\"{(t2-t1)/60}m\")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.387288Z","iopub.status.idle":"2024-05-20T10:13:45.387974Z","shell.execute_reply":"2024-05-20T10:13:45.387768Z","shell.execute_reply.started":"2024-05-20T10:13:45.387741Z"},"trusted":true},"outputs":[],"source":["# setting seed in each env\n","def set_random_seed(seed: int = 42, deterministic: bool = False):\n","    \"\"\"Set seeds\"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n","\n","# function to set tensor to device\n","def to_device(\n","    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n","    device: torch.device, *args, **kwargs\n","):\n","    if isinstance(tensors, tuple):\n","        return (t.to(device, *args, **kwargs) for t in tensors)\n","    elif isinstance(tensors, dict):\n","        return {\n","            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n","    else:\n","        return tensors.to(device, *args, **kwargs)"]},{"cell_type":"markdown","metadata":{},"source":["#### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.389232Z","iopub.status.idle":"2024-05-20T10:13:45.389914Z","shell.execute_reply":"2024-05-20T10:13:45.389708Z","shell.execute_reply.started":"2024-05-20T10:13:45.389683Z"},"trusted":true},"outputs":[],"source":["class Bird2024Dataset(Dataset):\n","    def __init__(\n","        self,\n","        image_paths: tp.Sequence[FilePath],\n","        labels: tp.Sequence[Label],\n","        transform: A.Compose,\n","    ):\n","        self.train_path_list = image_paths\n","        self.label_list = labels\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        # return total num of data\n","        return len(self.train_path_list)\n","    \n","    def __getitem__(self, index:int):\n","        # return data and target assosiated with index\n","        X = np.load(self.train_path_list[index])\n","        X = self._apply_transform(X)\n","        y = self.label_list[index] # y is ignored in test\n","\n","        return (X, y) # y is ignored in test\n","    \n","    def _apply_transform(self, img:np.ndarray):\n","        \"\"\"apply transform to image\"\"\"\n","        transformed = self.transform(image=img)\n","        img = transformed[\"image\"].float()\n","        return img\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["### Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.395153Z","iopub.status.idle":"2024-05-20T10:13:45.395839Z","shell.execute_reply":"2024-05-20T10:13:45.395629Z","shell.execute_reply.started":"2024-05-20T10:13:45.395608Z"},"trusted":true},"outputs":[],"source":["import timm\n","import torch\n","from torch import nn\n","\n","class BirdCLEF2024SpecModel(nn.Module):\n","\n","    def __init__(\n","            self,\n","            model_name: str,\n","            pretrained: bool,\n","            in_channels: int,\n","            num_classes: int,\n","        ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name=model_name, \n","            pretrained=pretrained,\n","            num_classes=num_classes, \n","            in_chans=in_channels\n","        )\n","\n","    def forward(self, x):\n","        h = self.model(x)      \n","\n","        return h"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.400994Z","iopub.status.idle":"2024-05-20T10:13:45.401653Z","shell.execute_reply":"2024-05-20T10:13:45.401464Z","shell.execute_reply.started":"2024-05-20T10:13:45.401439Z"},"trusted":true},"outputs":[],"source":["class FocalLossBCE(torch.nn.Module):\n","    def __init__(\n","        self,\n","        alpha: float = 0.25,\n","        gamma: float = 2,\n","        reduction: str = \"mean\",\n","        bce_weight: float = 1.0,\n","        focal_weight: float = 1.0,\n","    ):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n","        self.bce_weight = bce_weight\n","        self.focal_weight = focal_weight\n","\n","    def forward(self, inputs, targets):\n","        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n","            inputs=inputs,\n","            targets=targets,\n","            alpha=self.alpha,\n","            gamma=self.gamma,\n","            reduction=self.reduction,\n","        )\n","        bce_loss = self.bce(inputs, targets)\n","        return self.bce_weight * bce_loss + self.focal_weight * focall_loss"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"markdown","metadata":{},"source":["##### Function for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.407116Z","iopub.status.idle":"2024-05-20T10:13:45.407803Z","shell.execute_reply":"2024-05-20T10:13:45.407596Z","shell.execute_reply.started":"2024-05-20T10:13:45.40757Z"},"trusted":true},"outputs":[],"source":["def get_test_path_label(img_paths: list):\n","    \"\"\"Get file path and dummy target info.\"\"\"\n","    labels = np.full((len(img_paths), N_CLASSES), -1, dtype=\"float32\")\n","        \n","    test_data = {\n","        \"image_paths\": img_paths,\n","        \"labels\": [l for l in labels]}\n","    \n","    return test_data\n","\n","def get_test_transforms(CFG):\n","    test_transform = A.Compose([\n","        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n","        ToTensorV2(p=1.0)\n","    ])\n","    return test_transform"]},{"cell_type":"markdown","metadata":{},"source":["#### Inference loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.409036Z","iopub.status.idle":"2024-05-20T10:13:45.409692Z","shell.execute_reply":"2024-05-20T10:13:45.409504Z","shell.execute_reply.started":"2024-05-20T10:13:45.409478Z"},"trusted":true},"outputs":[],"source":["def run_inference_loop(model, loader, device):\n","    model.to(device)\n","    model.eval()\n","    pred_list = []\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            x = to_device(batch[0], device)\n","            y = model(x)\n","            pred_list.append(y.softmax(dim=1).detach().cpu().numpy())\n","        \n","    pred_arr = np.concatenate(pred_list)\n","    del pred_list\n","    return pred_arr"]},{"cell_type":"markdown","metadata":{},"source":["#### ONNX and OpenVINO"]},{"cell_type":"markdown","metadata":{},"source":["##### Functions that perform inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.412938Z","iopub.status.idle":"2024-05-20T10:13:45.413615Z","shell.execute_reply":"2024-05-20T10:13:45.413423Z","shell.execute_reply.started":"2024-05-20T10:13:45.413397Z"},"trusted":true},"outputs":[],"source":["# Functions that perform inference\n","def infernce_loop_openvino(\n","    infer_request: openvino.runtime.ie_api.InferRequest,\n","    loader: torch.utils.data.DataLoader,\n","    device\n","):\n","    pred_list = []\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            # get input\n","            input_data = to_device(batch[0], device).detach().cpu().numpy()\n","            # to ov tensor\n","            input_ov_tensor = ov.Tensor(array=input_data, shared_memory=True)\n","            # set input tensor\n","            infer_request.set_input_tensor(input_ov_tensor)\n","            # inference\n","            output_numpy = infer_request.infer()[\"output\"]\n","            # to probability\n","            output_softmax = softmax(output_numpy, axis=1)\n","            # stack the prediction\n","            pred_list.append(output_softmax)\n","    \n","    # If asynchronous processing is used effectively, there is a potential to further speed up the inference workflow.\n","    # infer_request.start_async()\n","    # infer_request.wait()\n","    # output_numpy = infer_request.get_output_tensor().data\n","    pred_arr = np.concatenate(pred_list)\n","    del pred_list\n","    return pred_arr"]},{"cell_type":"markdown","metadata":{},"source":["#### Converting models to openvino"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.414946Z","iopub.status.idle":"2024-05-20T10:13:45.415627Z","shell.execute_reply":"2024-05-20T10:13:45.41543Z","shell.execute_reply.started":"2024-05-20T10:13:45.415402Z"},"trusted":true},"outputs":[],"source":["# converting models to openvino\n","def convert_pytorch_to_openvino(device):\n","    for fold_id in range(CFG.n_folds):\n","        # load model\n","        model_path = TRAINED_MODEL / f\"best_model_fold{fold_id}.pth\"\n","        model = BirdCLEF2024SpecModel(\n","            model_name=CFG.model_name, pretrained=False, num_classes=N_CLASSES, in_channels=1\n","        )\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        model.eval()\n","        # export to onnx\n","        torch.onnx.export(model,\n","                      CFG.DUMMY_INPUT_TENSOR,\n","                      CFG.OUTPUT_DIR_ONNX / f\"fp32_fold{fold_id}.onnx\",\n","                      opset_version=15,\n","                      input_names=['input'],\n","                      output_names=['output']\n","        )\n","\n","        # convert model to openvino\n","        ov_model = ov.convert_model(CFG.OUTPUT_DIR_ONNX / f\"fp32_fold{fold_id}.onnx\",\n","                                input=[('input', CFG.INPUT_SHAPE)],)\n","        # save model\n","        ov.save_model(ov_model, CFG.OUTPUT_DIR_OV / f\"fp32_fold{fold_id}.xml\", compress_to_fp16=False)\n","        \n","convert_pytorch_to_openvino(device=torch.device(CFG.device))"]},{"cell_type":"markdown","metadata":{},"source":["#### Executing inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.416948Z","iopub.status.idle":"2024-05-20T10:13:45.417616Z","shell.execute_reply":"2024-05-20T10:13:45.417425Z","shell.execute_reply.started":"2024-05-20T10:13:45.417405Z"},"trusted":true},"outputs":[],"source":["def execute_inference():\n","    img_paths = [filepath for filepath in TEST_IMAGE.iterdir()]\n","    if CFG.simple_inferring:\n","        if len(img_paths) > CFG.n_simple:\n","            img_paths = img_paths[:CFG.n_simple]\n","            \n","    IDs = ['soundscape_' + filepath.stem for filepath in TEST_IMAGE.iterdir()]\n","    test_preds_arr = np.zeros((CFG.n_folds, len(img_paths), N_CLASSES))\n","\n","    test_path_label = get_test_path_label(img_paths)\n","    test_transform = get_test_transforms(CFG)\n","    test_dataset = Bird2024Dataset(**test_path_label, transform=test_transform)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n","\n","    device = torch.device(CFG.device)\n","    \n","    # Make an openvino core object\n","    core = ov.Core()\n","    for fold_id in range(CFG.n_folds):\n","        print(f\"\\n[fold {fold_id}]\")\n","\n","        ##  ・When using an external model\n","        # model_path = TRAINED_MODEL / f\"best_model_fold{fold_id}.pth\"\n","        # model = BirdCLEF2024SpecModel(\n","        #     model_name=CFG.model_name, pretrained=False, num_classes=N_CLASSES, in_channels=1)\n","        # model.load_state_dict(torch.load(model_path, map_location=device))\n","        \n","        # Cmpile\n","        compiled_model = core.compile_model(CFG.OUTPUT_DIR_OV / f\"fp32_fold{fold_id}.xml\", device_name='CPU')\n","        # Make an inference request\n","        infer_request = compiled_model.create_infer_request()\n","        # Make an inference\n","        test_pred = infernce_loop_openvino(infer_request, test_loader, device)\n","\n","        ##  ・When using an external model\n","        # test_pred = run_inference_loop(model, test_loader, device)\n","        test_preds_arr[fold_id] = test_pred\n","\n","        del compiled_model, infer_request, test_pred\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    return test_preds_arr, IDs\n","\n","# execute\n","t1 = time()\n","test_preds_arr, IDs = execute_inference()\n","t2 = time()\n","print('inference time: ', f\"{(t2-t1)/60}m\")"]},{"cell_type":"markdown","metadata":{},"source":["##### Creating sample submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.418883Z","iopub.status.idle":"2024-05-20T10:13:45.419533Z","shell.execute_reply":"2024-05-20T10:13:45.419338Z","shell.execute_reply.started":"2024-05-20T10:13:45.419312Z"},"trusted":true},"outputs":[],"source":["def make_submission(test_preds_arr, IDs):\n","    # average of each fold's model\n","    test_pred = test_preds_arr.mean(axis=0)\n","\n","    # make id column\n","    IDs = pd.DataFrame(\n","        IDs, columns=['row_id']\n","    )\n","\n","    # make prediction colmuns\n","    test_pred_df = pd.DataFrame(\n","        test_pred, columns=CLASSES\n","    )\n","\n","    # concat\n","    sub = pd.concat([IDs, test_pred_df], axis=1)\n","    \n","    # Sort\n","    # Extract the middle and last numbers from each string in the column\n","    sub['middle_number'] = sub['row_id'].str.extract(r'_([0-9]+)_')\n","    sub['end_number'] = sub['row_id'].str.extract(r'_(\\d+)$').astype(int)\n","\n","    # Sort by 'middle_number' first,'end_number' second\n","    sub = sub.sort_values(by=['middle_number', 'end_number'], ignore_index=True)\n","    sub = sub.drop(columns=['middle_number', 'end_number'])\n","    sub = sub.dropna(axis=0,how='any')\n","\n","    # make submission\n","    sub.to_csv(\"submission.csv\", index=False)\n","    display(sub.head())\n","    print(sub.shape)\n","    print(sub.iloc[0][CLASSES].sum())\n","    \n","    return sub\n","    \n","sub = make_submission(test_preds_arr, IDs)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### check the batch data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:13:45.424761Z","iopub.status.idle":"2024-05-20T10:13:45.425481Z","shell.execute_reply":"2024-05-20T10:13:45.425244Z","shell.execute_reply.started":"2024-05-20T10:13:45.425223Z"},"trusted":true},"outputs":[],"source":["if CFG.show:\n","    img_paths = [filepath for filepath in TEST_IMAGE.iterdir()]\n","    if CFG.simple_inferring:\n","        if len(img_paths) > CFG.n_simple:\n","            img_paths = img_paths[:CFG.n_simple]\n","\n","    IDs = ['soundscape_' + filepath.stem for filepath in TEST_IMAGE.iterdir()]\n","    test_preds_arr = np.zeros((CFG.n_folds, len(img_paths), N_CLASSES))\n","\n","    test_path_label = get_test_path_label(img_paths)\n","    test_transform = get_test_transforms(CFG)\n","    test_dataset = Bird2024Dataset(**test_path_label, transform=test_transform)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n","\n","    def show_batch(ds, row=3, col=3):\n","        fig = plt.figure(figsize=(10, 10))\n","        img_index = np.random.randint(0, len(ds)-1, row*col)\n","\n","        for i in range(len(img_index)):\n","            img, label = ds[img_index[i]]\n","\n","            if isinstance(img, torch.Tensor):\n","                img = img.detach().numpy()\n","                img = np.squeeze(img)\n","\n","            ax = fig.add_subplot(row, col, i + 1, xticks=[], yticks=[])\n","            ax.imshow(img, cmap='jet')\n","        plt.tight_layout()\n","        plt.show()\n","\n","    show_batch(test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["END"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8068726,"sourceId":70203,"sourceType":"competition"},{"datasetId":4745108,"sourceId":8048860,"sourceType":"datasetVersion"},{"datasetId":4746211,"sourceId":8048895,"sourceType":"datasetVersion"},{"datasetId":4746338,"sourceId":8048897,"sourceType":"datasetVersion"},{"datasetId":4836781,"sourceId":8172274,"sourceType":"datasetVersion"},{"datasetId":4963066,"isSourceIdPinned":true,"sourceId":8353101,"sourceType":"datasetVersion"},{"datasetId":4963056,"sourceId":8446576,"sourceType":"datasetVersion"},{"datasetId":4963063,"sourceId":8449682,"sourceType":"datasetVersion"},{"datasetId":4984448,"sourceId":8545486,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":31.037946,"end_time":"2024-04-13T08:15:36.967243","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-13T08:15:05.929297","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
