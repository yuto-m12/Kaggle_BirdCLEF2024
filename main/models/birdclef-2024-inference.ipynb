{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005009,"end_time":"2024-04-13T08:15:08.535649","exception":false,"start_time":"2024-04-13T08:15:08.53064","status":"completed"},"tags":[]},"source":["# First\n","\n","This is a inference notebook.<br>\n","Training and Making dataset are existing."]},{"cell_type":"markdown","metadata":{},"source":["# 0. Create the image from test data\n","I have to create image like spectrogram for each 5sec in test soundscape.\n","> ・Test data description  \n","> When a Notebook is submitted, approximately 1100 4-minute pieces of audio data will be placed in this directory. The filename is random, but has a common name: soundscape_xxxxxx.ogg.\n","It will take approximately 5 minutes to download all data."]},{"cell_type":"markdown","metadata":{},"source":["#### Import "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.01921Z","iopub.status.idle":"2024-06-06T03:50:13.019588Z","shell.execute_reply":"2024-06-06T03:50:13.019407Z","shell.execute_reply.started":"2024-06-06T03:50:13.019393Z"},"trusted":true},"outputs":[],"source":["from time import time\n","t1 = time()\n","# onnxsim-0.4.36\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxsim-0.4.36/onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","\n","# onnxruntime-1.17.3\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/humanfriendly-10.0-py2.py3-none-any.whl\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/coloredlogs-15.0.1-py2.py3-none-any.whl\n","# !pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxruntime-1.17.3/onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n","\n","# onnxconverter-common-1.14.0\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxconverter-common-1.14.0/protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnxconverter-common-1.14.0/onnxconverter_common-1.14.0-py2.py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/fastjsonschema-2.17.1/fastjsonschema-2.17.1-py3-none-any.whl\n","\n","# openvino-dev-2024.0.0\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/onnx-1.15.0/onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/networkx-3.1-py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino_telemetry-2024.1.0-py3-none-any.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino-2024.0.0-14509-cp310-cp310-manylinux2014_x86_64.whl\n","!pip install --no-index /kaggle/input/birdclef2024-openvino-onnxruntime/openvino-dev-2024.0.0/openvino_dev-2024.0.0-14509-py3-none-any.whl\n","\n","t2 = time()\n","print('Import time: ', f\"{(t2-t1)/60}m\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.020648Z","iopub.status.idle":"2024-06-06T03:50:13.021056Z","shell.execute_reply":"2024-06-06T03:50:13.020884Z","shell.execute_reply.started":"2024-06-06T03:50:13.020868Z"},"trusted":true},"outputs":[],"source":["# Basic\n","import sys\n","import os\n","import gc\n","import copy\n","import yaml\n","import random\n","import shutil\n","from time import time\n","import gzip\n","import bz2\n","import ast\n","\n","# Python\n","import numpy as np\n","import pandas as pd\n","import pandas.api.types\n","import pickle\n","import pywt\n","import librosa\n","import librosa.display\n","from pathlib import Path\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GroupKFold\n","from sklearn.model_selection import StratifiedGroupKFold\n","import sklearn.metrics\n","import matplotlib.pyplot as plt \n","import plotly.express as px\n","import typing as tp\n","import cv2\n","from scipy.special import softmax\n","from glob import glob\n","\n","# Notebook\n","from IPython.display import Audio\n","from tqdm.notebook import tqdm\n","\n","# PyTorch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.cuda import amp\n","import timm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# For faster inference\n","import onnx\n","# import onnxruntime as rt\n","from onnxconverter_common import float16\n","import openvino\n","import openvino as ov\n","\n","\n","\n","\n","# Use one device only\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","# To show all af column\n","pd.set_option('display.max_columns', None)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.02228Z","iopub.status.idle":"2024-06-06T03:50:13.022687Z","shell.execute_reply":"2024-06-06T03:50:13.022465Z","shell.execute_reply.started":"2024-06-06T03:50:13.022451Z"},"trusted":true},"outputs":[],"source":["# test dir\n","TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/test_soundscapes')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.025049Z","iopub.status.idle":"2024-06-06T03:50:13.025408Z","shell.execute_reply":"2024-06-06T03:50:13.02524Z","shell.execute_reply.started":"2024-06-06T03:50:13.025226Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    def __init__(self):\n","        # check the test data exist or not\n","        self.TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/test_soundscapes')\n","        self.NO_SOUNDSCAPE = False\n","        if 1 >= len(os.listdir(self.TEST_SOUNDSCAPE)):\n","            self.TEST_SOUNDSCAPE = Path('/kaggle/input/birdclef-2024/train_audio/asbfly')\n","            self.NO_SOUNDSCAPE = True\n","\n","    model_name = \"efficientnet_b0.ra_in1k\"  # model \n","    img_size = 224                          # input size\n","    n_folds = 5                             # number of fold\n","    interpolation = cv2.INTER_AREA          # specifying method of interpolation(dfault is cv2.INTER_LINEAR)\n","    max_epoch = 9                           # number of max epoch. 1epoch means going around the training dataset.\n","#     batch_size = 32                        # train batch size. Number of samples passed to the network in one training step\n","    batch_size = 1                          # test batch size. openvino can't response flexibility to leftovers of number of batches \n","    lr = 1.0e-03                            # learning rate. determine step size when updating model's weight\n","    weight_decay = 1.0e-02                  # weight decay. Append regularization term for prevent over fitting\n","    es_patience = 5                         # Early Stopping number of epoch\n","    seed = 1086                             # seed\n","    deterministic = True                    # deterministic or not\n","    enable_amp = False                      # Automatic Mixed Precision\n","#     device = \"cuda\"                        # Device to use training. \"cuda\" is NVIDIA GPU\n","    device = \"cpu\"                          # Device to use inference.\n","    \n","    # related with test \n","    simple_training = True                  # only use few data with training, be enable in training\n","    simple_inferring = False                # only use few data with inferring\n","    n_simple = 100                          # number of data with simple training\n","    test = True                             # when inference\n","    show = False                            # show few batch data at the end\n","    \n","    # related with Data Loading \n","    MELSPEC_H = 128                         # Horizontal melspectrogram resolution\n","    TOP_DB = 100                            # Maximum decibel to clip audio to\n","    MIN_RATING = 0.0                        # Minimum rating\n","    SR = 32000                              # Sample rate as provided in competition description\n","    N_FFT = 2000\n","    HOP_LENGTH = 500\n","    \n","    # related with faster inference \n","    INPUT_SHAPE: list[int] = [1, 1, 224, 224]\n","    DUMMY_INPUT_TENSOR: torch.Tensor = torch.randn(*INPUT_SHAPE)\n","    DUMMY_INPUT_NUMPY_FP32: np.ndarray = DUMMY_INPUT_TENSOR.numpy()\n","    DUMMY_INPUT_NUMPY_FP16: np.ndarray = DUMMY_INPUT_NUMPY_FP32.astype(np.float16)\n","    OUTPUT_DIR_ONNX: Path = Path('./model/onnx')\n","    OUTPUT_DIR_OV: Path = Path('./model/ov')\n","        \n","    # submission info\n","    sample_submission = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv')\n","    CLASSES = sample_submission.columns[1:].values\n","    N_CLASSES = len(CLASSES)\n","\n","CFG = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.026587Z","iopub.status.idle":"2024-06-06T03:50:13.026967Z","shell.execute_reply":"2024-06-06T03:50:13.026804Z","shell.execute_reply.started":"2024-06-06T03:50:13.026789Z"},"trusted":true},"outputs":[],"source":["# model output dir\n","CFG.OUTPUT_DIR_ONNX.mkdir(parents=True, exist_ok=True)\n","CFG.OUTPUT_DIR_OV.mkdir(parents=True, exist_ok=True)\n","\n","# type elements\n","FilePath = tp.Union[str, Path]\n","Label = tp.Union[int, float, np.ndarray]"]},{"cell_type":"markdown","metadata":{},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.028633Z","iopub.status.idle":"2024-06-06T03:50:13.029017Z","shell.execute_reply":"2024-06-06T03:50:13.028847Z","shell.execute_reply.started":"2024-06-06T03:50:13.028831Z"},"trusted":true},"outputs":[],"source":["# audio file\n","KAGGLE_TRAIN = '/kaggle/input/birdclef-2024/train_audio'\n","ADDED_TRAIN = '/kaggle/input/birdclef2024-additional-mp3/additional_audio'\n","ADDED_TRAIN_1 = '/kaggle/input/birdclef2024-additional-wav-1/additional_audio-1'\n","ADDED_TRAIN_2 = '/kaggle/input/birdclef2024-additional-wav-2/additional_audio-2'\n","TEST_SOUNDSCAPE = CFG.TEST_SOUNDSCAPE\n","os.makedirs(KAGGLE_TRAIN, exist_ok=True)\n","\n","# to save image from audio\n","SAVE_TRAIN = '/kaggle/working/train_image'\n","SAVE_TEST = '/kaggle/working/test_image'\n","\n","# image input\n","TRAIN_IMAGE = Path('/kaggle/input/bird2024-melspec-v6/train_image/melspec')\n","TEST_IMAGE = Path('/kaggle/working/test_image/melspec')\n","\n","# model trained\n","TRAINED_MODEL = Path('/kaggle/input/birdcref-2024-introduction-withtraining-train-v2')\n","\n","class preprocessing():\n","    def __init__(self, AUDIO_DIRECTORY, SAVE_DIRECTORY, view=False, test=CFG.test):\n","        # config\n","        self.AUDIO_DIRECTORY = AUDIO_DIRECTORY\n","        self.SAVE_DIRECTORY = SAVE_DIRECTORY\n","        self.view = view\n","        self.test = test\n","        \n","        # make directory\n","        func_names = [method for method in dir(self) if callable(getattr(self, method)) and method.startswith(\"func\")]\n","        print(func_names)\n","        os.makedirs(self.SAVE_DIRECTORY, exist_ok=True)\n","        for func_name in func_names:\n","            func = func_name.split('_')[-1]\n","            os.makedirs(self.SAVE_DIRECTORY + '/' + func, exist_ok=True)\n","    \n","    # load data\n","    def load_wave(self, audio_filepath, offset=0):\n","        # pick up 5 seconds\n","        if CFG.NO_SOUNDSCAPE:\n","            offset = 0\n","        self.y, _ = librosa.load(audio_filepath, sr=CFG.SR , offset=offset, duration=5)\n","        self.sr = CFG.SR\n","    \n","    def normalize(self, data: np.ndarray):\n","        data = data.astype(np.single)\n","        # Normalize min to max\n","        data = data - data.min()\n","        # Normalize 0 to 255\n","        data = (data / data.max() * 255).astype(np.uint8)\n","        \n","        return data\n","        \n","    # apply and save\n","    def apply_func(self, function):\n","        species_list = os.listdir(self.AUDIO_DIRECTORY)\n","        if not self.test:\n","            for species in species_list:\n","                species_path = self.AUDIO_DIRECTORY + '/' + species\n","                audio_file_list = os.listdir(species_path)\n","                for audio_file in audio_file_list:\n","                    audio_filepath = species_path + '/' + audio_file\n","                    self.load_wave(audio_filepath) # load audio\n","                    output = function() # apply function\n","                    output = self.normalize(output)\n","                    \n","\n","                    SAVE_DIRECTORY = Path(self.SAVE_DIRECTORY + '/' + function.__name__.split('_')[-1] + '/' + species)\n","                    SAVE_DIRECTORY.mkdir(exist_ok=True)\n","                    SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file.split('.')[0]}.npy\"\n","                    np.save(SAVE_PATH, output)\n","                    del output\n","\n","        if self.test:\n","            audio_directory_path = self.AUDIO_DIRECTORY\n","            audio_length = int(4*60) # second\n","            audio_offset_unit_max = int(audio_length / 5)\n","            for audio_file_path in audio_directory_path.glob('*.ogg'):\n","                    for audio_offset_unit in range(audio_offset_unit_max):\n","                        audio_offset = audio_offset_unit * 5\n","                        self.load_wave(str(audio_file_path), audio_offset) # load audio\n","                        output = function() # apply function\n","                        output = self.normalize(output)\n","\n","                        SAVE_DIRECTORY = Path(self.SAVE_DIRECTORY) / function.__name__.split('_')[-1]\n","                        SAVE_DIRECTORY.mkdir(exist_ok=True)\n","                        if CFG.NO_SOUNDSCAPE:\n","                            SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file_path.stem.replace('XC','')}_{audio_offset+5}.npy\" # [soundscape_id]_[end_time].npy\n","                        else:\n","                            SAVE_PATH = SAVE_DIRECTORY  / f\"{audio_file_path.stem.replace('soundscape_','')}_{audio_offset+5}.npy\" # [soundscape_id]_[end_time].npy\n","                        np.save(SAVE_PATH, output)\n","\n","                        del output\n","\n","          \n","        \n","    def save_as_picke_gzip(self, data, filepath):       \n","        with gzip.open(filepath, 'wb') as f:\n","            pickle.dump(data, f)\n","            \n","    def func_waveform(self):        \n","        if self.view:\n","            print('waveform shape: ', self.y.shape)\n","            display(Audio(self.y, rate=self.sr))\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.waveshow(self.y, sr=self.sr)\n","            plt.title('Waveform')\n","            plt.xlabel('Time (s)')\n","            plt.ylabel('Amplitude')\n","            plt.show()\n","        return self.y\n","    \n","    def func_spec(self):\n","        spec = librosa.amplitude_to_db(\n","            np.abs(librosa.stft(self.y)), \n","            ref=np.max,\n","        )\n","        \n","        if self.view:\n","            print('spec shape: ', spec.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(spec, sr=self.sr, x_axis='time', y_axis='log')\n","            plt.colorbar(format='%+2.0f dB')\n","            plt.title('Spectrogram')\n","            plt.show()\n","        return spec\n","    \n","    def func_melspec(self):\n","        melspec = librosa.feature.melspectrogram(\n","            y=self.y, \n","            sr=CFG.SR,                 # sample rate\n","            n_fft=CFG.N_FFT,           # number of samples in window \n","            hop_length=CFG.HOP_LENGTH, # step size of window\n","            n_mels=CFG.MELSPEC_H,      # horizontal resolution from fmin→fmax in log scale\n","            fmin=40,                   # minimum frequency\n","            fmax=16000,                # maximum frequency\n","            power=2.0,                 # intensity^power for log scale\n","        )\n","        melspec = librosa.power_to_db(melspec, ref=np.max)\n","        \n","        if self.view:\n","            print('melspec shape: ', melspec.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(melspec, sr=self.sr, x_axis='time', y_axis='mel')\n","            plt.colorbar(format='%+2.0f dB')\n","            plt.title('Mel Spectrogram')\n","            plt.show()\n","        return melspec\n","    \n","    def func_scalogram(self):\n","        scales = pywt.central_frequency('cmor') / np.linspace(1, 100, 100) * self.sr\n","        cwtmatr, freqs = pywt.cwt(self.y, scales, 'cmor', sampling_period=1/self.sr)\n","        \n","        if self.view:\n","            print('scarogram shape: ', cwtmatr.shape)\n","            plt.figure(figsize=(10, 4))\n","            plt.imshow(abs(cwtmatr), aspect='auto', extent=[0, len(self.y) / self.sr, 1, 100], cmap='jet', origin='lower')\n","            plt.colorbar()\n","            plt.title('Scalogram')\n","            plt.xlabel('Time (s)')\n","            plt.ylabel('Scale')\n","            plt.show()\n","        # to real value\n","        return abs(cwtmatr)\n","\n","    def func_chromagram(self):\n","        C = librosa.feature.chroma_cqt(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('chromagram shape: ', C.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(C, sr=self.sr, x_axis='time', y_axis='chroma', cmap='coolwarm')\n","            plt.colorbar()\n","            plt.title('Chromagram')\n","            plt.show()\n","        return C\n","\n","    \n","    def func_mfcc(self): \n","        mfcc = librosa.feature.mfcc(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('mfcc shape: ', mfcc.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(mfcc, sr=self.sr, x_axis='time')\n","            plt.ylabel('MFCC coeffs')\n","            plt.colorbar()\n","            plt.title('MFCC')\n","            plt.show()\n","        return mfcc\n","\n","    def func_spectralcontrast(self):\n","        contrast = librosa.feature.spectral_contrast(y=self.y, sr=self.sr)\n","        \n","        if self.view:\n","            print('contrast shape: ', contrast.shape)\n","            plt.figure(figsize=(10, 4))\n","            librosa.display.specshow(contrast, x_axis='time')\n","            plt.colorbar()\n","            plt.ylabel('Frequency bands')\n","            plt.title('Spectral Contrast')\n","            plt.show()\n","        return contrast\n","\n","    \n","    def execute(self):\n","        func_list = [\n","#             self.func_waveform,\n","#             self.func_spec,\n","            self.func_melspec,\n","#             self.func_scalogram,\n","#             self.func_chromagram,\n","#             self.func_mfcc,\n","#             self.func_spectralcontrast,\n","        ]\n","        for func in func_list:\n","            self.apply_func(func)\n","\n","# ・ Define preprocessing class\n","# preprocessing_kaggle = preprocessing(KAGGLE_TRAIN, SAVE_TRAIN, view=True)\n","# preprocessing_added_train = preprocessing(ADDED_TRAIN, SAVE_TRAIN)\n","# preprocessing_added_train_1 = preprocessing(ADDED_TRAIN_1, SAVE_TRAIN)\n","# preprocessing_added_train_2 = preprocessing(ADDED_TRAIN_2, SAVE_TRAIN)\n","preprocessing_test = preprocessing(TEST_SOUNDSCAPE, SAVE_TEST, test=True)\n","\n","t1 = time()\n","# ・ Execute preprocessing\n","# preprocessing_kaggle.execute()\n","# preprocessing_added_train.execute()\n","# preprocessing_added_train_1.execute()\n","# preprocessing_added_train_2.execute()\n","preprocessing_test.execute()\n","t2 = time()\n","print('Preprocessing time: ', f\"{(t2-t1)/60}m\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.071252Z","iopub.status.idle":"2024-06-06T03:50:13.071645Z","shell.execute_reply":"2024-06-06T03:50:13.071459Z","shell.execute_reply.started":"2024-06-06T03:50:13.071444Z"},"trusted":true},"outputs":[],"source":["# setting seed in each env\n","def set_random_seed(seed: int = 42, deterministic: bool = False):\n","    \"\"\"Set seeds\"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n","\n","# function to set tensor to device\n","def to_device(\n","    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n","    device: torch.device, *args, **kwargs\n","):\n","    if isinstance(tensors, tuple):\n","        return (t.to(device, *args, **kwargs) for t in tensors)\n","    elif isinstance(tensors, dict):\n","        return {\n","            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n","    else:\n","        return tensors.to(device, *args, **kwargs)"]},{"cell_type":"markdown","metadata":{},"source":["#### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.073298Z","iopub.status.idle":"2024-06-06T03:50:13.073851Z","shell.execute_reply":"2024-06-06T03:50:13.073582Z","shell.execute_reply.started":"2024-06-06T03:50:13.073538Z"},"trusted":true},"outputs":[],"source":["class Bird2024Dataset(Dataset):\n","    def __init__(\n","        self,\n","        image_paths: tp.Sequence[FilePath],\n","        labels: tp.Sequence[Label],\n","        transform: A.Compose,\n","    ):\n","        self.train_path_list = image_paths\n","        self.label_list = labels\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        # return total num of data\n","        return len(self.train_path_list)\n","    \n","    def __getitem__(self, index:int):\n","        # return data and target assosiated with index\n","        X = np.load(self.train_path_list[index])\n","        X = self._apply_transform(X)\n","        y = self.label_list[index]\n","#         y = LABEL2NUM[y] # Only training need this\n","\n","        return (X, y)\n","    \n","    def _apply_transform(self, img:np.ndarray):\n","        \"\"\"apply transform to image\"\"\"\n","        transformed = self.transform(image=img)\n","        img = transformed[\"image\"].float()\n","        return img\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["# Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.079307Z","iopub.status.idle":"2024-06-06T03:50:13.079684Z","shell.execute_reply":"2024-06-06T03:50:13.079497Z","shell.execute_reply.started":"2024-06-06T03:50:13.079483Z"},"trusted":true},"outputs":[],"source":["class BirdCLEF2024SpecModel(nn.Module):\n","\n","    def __init__(\n","            self,\n","            model_name: str,\n","            pretrained: bool,\n","            in_channels: int,\n","            num_classes: int,\n","        ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name=model_name, \n","            pretrained=pretrained,\n","            num_classes=num_classes, \n","            in_chans=in_channels\n","        )\n","\n","    def forward(self, x):\n","        h = self.model(x)      \n","\n","        return h"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"markdown","metadata":{},"source":["##### Function for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.088564Z","iopub.status.idle":"2024-06-06T03:50:13.088964Z","shell.execute_reply":"2024-06-06T03:50:13.088783Z","shell.execute_reply.started":"2024-06-06T03:50:13.088766Z"},"trusted":true},"outputs":[],"source":["def get_test_path_label(img_paths: list):\n","    \"\"\"Get file path and dummy target info.\"\"\"\n","    labels = np.full((len(img_paths), CFG.N_CLASSES), -1, dtype=\"float32\")\n","        \n","    test_data = {\n","        \"image_paths\": img_paths,\n","        \"labels\": [l for l in labels]}\n","    \n","    return test_data\n","\n","def get_test_transforms(CFG):\n","    test_transform = A.Compose([\n","        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size, interpolation = CFG.interpolation),\n","        ToTensorV2(p=1.0)\n","    ])\n","    return test_transform"]},{"cell_type":"markdown","metadata":{},"source":["#### Inference loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.090361Z","iopub.status.idle":"2024-06-06T03:50:13.090737Z","shell.execute_reply":"2024-06-06T03:50:13.090573Z","shell.execute_reply.started":"2024-06-06T03:50:13.090535Z"},"trusted":true},"outputs":[],"source":["def run_inference_loop(model, loader, device):\n","    model.to(device)\n","    model.eval()\n","    pred_list = []\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            x = to_device(batch[0], device)\n","            y = model(x)\n","            pred_list.append(y.softmax(dim=1).detach().cpu().numpy())\n","        \n","    pred_arr = np.concatenate(pred_list)\n","    del pred_list\n","    return pred_arr"]},{"cell_type":"markdown","metadata":{},"source":["#### ONNX and OpenVINO"]},{"cell_type":"markdown","metadata":{},"source":["##### Functions that perform inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.093605Z","iopub.status.idle":"2024-06-06T03:50:13.094017Z","shell.execute_reply":"2024-06-06T03:50:13.09383Z","shell.execute_reply.started":"2024-06-06T03:50:13.093815Z"},"trusted":true},"outputs":[],"source":["# Functions that perform inference\n","def infernce_loop_openvino(\n","    infer_request: openvino.runtime.ie_api.InferRequest,\n","    loader: torch.utils.data.DataLoader,\n","    device\n","):\n","    pred_list = []\n","    with torch.no_grad():\n","        for batch in tqdm(loader):\n","            # get input\n","            input_data = to_device(batch[0], device).detach().cpu().numpy()\n","            # to ov tensor\n","            input_ov_tensor = ov.Tensor(array=input_data, shared_memory=True)\n","            # set input tensor\n","            infer_request.set_input_tensor(input_ov_tensor)\n","            # inference\n","            output_numpy = infer_request.infer()[\"output\"]\n","            # to probability\n","            output_softmax = softmax(output_numpy, axis=1)\n","            # stack the prediction\n","            pred_list.append(output_softmax)\n","    \n","    # If asynchronous processing is used effectively, there is a potential to further speed up the inference workflow.\n","    # infer_request.start_async()\n","    # infer_request.wait()\n","    # output_numpy = infer_request.get_output_tensor().data\n","    pred_arr = np.concatenate(pred_list)\n","    del pred_list\n","    return pred_arr"]},{"cell_type":"markdown","metadata":{},"source":["#### Converting models to openvino"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.094985Z","iopub.status.idle":"2024-06-06T03:50:13.095345Z","shell.execute_reply":"2024-06-06T03:50:13.095188Z","shell.execute_reply.started":"2024-06-06T03:50:13.095174Z"},"trusted":true},"outputs":[],"source":["# converting models to openvino\n","def convert_pytorch_to_openvino(device):\n","    for fold_id in range(CFG.n_folds):\n","        # load model\n","        model_path = TRAINED_MODEL / f\"best_model_fold{fold_id}.pth\"\n","        model = BirdCLEF2024SpecModel(\n","            model_name=CFG.model_name, pretrained=False, num_classes=CFG.N_CLASSES, in_channels=1\n","        )\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        model.eval()\n","        # export to onnx\n","        torch.onnx.export(model,\n","                      CFG.DUMMY_INPUT_TENSOR,\n","                      CFG.OUTPUT_DIR_ONNX / f\"fp32_fold{fold_id}.onnx\",\n","                      opset_version=15,\n","                      input_names=['input'],\n","                      output_names=['output']\n","        )\n","\n","        # convert model to openvino\n","        ov_model = ov.convert_model(CFG.OUTPUT_DIR_ONNX / f\"fp32_fold{fold_id}.onnx\",\n","                                input=[('input', CFG.INPUT_SHAPE)],)\n","        # save model\n","        ov.save_model(ov_model, CFG.OUTPUT_DIR_OV / f\"fp32_fold{fold_id}.xml\", compress_to_fp16=False)\n","        \n","convert_pytorch_to_openvino(device=torch.device(CFG.device))"]},{"cell_type":"markdown","metadata":{},"source":["#### Executing inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.096782Z","iopub.status.idle":"2024-06-06T03:50:13.097124Z","shell.execute_reply":"2024-06-06T03:50:13.09697Z","shell.execute_reply.started":"2024-06-06T03:50:13.096955Z"},"trusted":true},"outputs":[],"source":["def execute_inference():\n","    img_paths = [filepath for filepath in TEST_IMAGE.iterdir()]\n","    if CFG.simple_inferring:\n","        if len(img_paths) > CFG.n_simple:\n","            img_paths = img_paths[:CFG.n_simple]\n","            \n","    IDs = [\"soundscape_\" + filepath.stem for filepath in TEST_IMAGE.iterdir()]\n","    test_preds_arr = np.zeros((2, len(img_paths), CFG.N_CLASSES))\n","\n","    test_path_label = get_test_path_label(img_paths)\n","    test_transform = get_test_transforms(CFG)\n","    test_dataset = Bird2024Dataset(**test_path_label, transform=test_transform)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n","\n","    device = torch.device(CFG.device)\n","    \n","    # Make an openvino core object\n","    core = ov.Core()\n","    for _ in range(1):\n","        fold_id = 1\n","        print(f\"\\n[fold {fold_id}]\")\n","\n","        ##  ・When using an external model\n","        # model_path = TRAINED_MODEL / f\"best_model_fold{fold_id}.pth\"\n","        # model = BirdCLEF2024SpecModel(\n","        #     model_name=CFG.model_name, pretrained=False, num_classes=CFG.N_CLASSES, in_channels=1)\n","        # model.load_state_dict(torch.load(model_path, map_location=device))\n","        \n","        # Cmpile\n","        compiled_model = core.compile_model(CFG.OUTPUT_DIR_OV / f\"fp32_fold{fold_id}.xml\", device_name='CPU')\n","        # Make an inference request\n","        infer_request = compiled_model.create_infer_request()\n","        # Make an inference\n","        test_pred = infernce_loop_openvino(infer_request, test_loader, device)\n","\n","        ##  ・When using an external model\n","        # test_pred = run_inference_loop(model, test_loader, device)\n","        test_preds_arr[fold_id] = test_pred\n","\n","        del compiled_model, infer_request, test_pred\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    return test_preds_arr, IDs\n","\n","# execute\n","t1 = time()\n","test_preds_arr, IDs = execute_inference()\n","t2 = time()\n","print('inference time: ', f\"{(t2-t1)/60}m\")"]},{"cell_type":"markdown","metadata":{},"source":["##### Creating sample submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-06T03:50:13.098105Z","iopub.status.idle":"2024-06-06T03:50:13.098446Z","shell.execute_reply":"2024-06-06T03:50:13.098287Z","shell.execute_reply.started":"2024-06-06T03:50:13.09827Z"},"trusted":true},"outputs":[],"source":["def make_submission(test_preds_arr, IDs):\n","    # average of each fold's model\n","    test_pred = test_preds_arr.mean(axis=0)\n","\n","    # make id column\n","    IDs = pd.DataFrame(\n","        IDs, columns=['row_id']\n","    )\n","\n","    # make prediction colmuns\n","    test_pred_df = pd.DataFrame(\n","        test_pred, columns=CFG.CLASSES\n","    )\n","\n","    # concat\n","    sub = pd.concat([IDs, test_pred_df], axis=1)\n","    \n","    # Sort\n","    # Extract the middle and last numbers from each string in the column\n","    sub['middle_number'] = sub['row_id'].str.extract(r'_([0-9]+)_')\n","    sub['end_number'] = sub['row_id'].str.extract(r'_(\\d+)$').astype(int)\n","\n","    # Sort by 'middle_number' first, then 'end_number'\n","    sub = sub.sort_values(by=['middle_number', 'end_number'], ignore_index=True)\n","    sub = sub.drop(columns=['middle_number', 'end_number'])\n","    sub = sub.dropna(axis=0,how='any')\n","\n","    # make submission\n","    sub.to_csv(\"submission.csv\", index=False)\n","    display(sub.head())\n","    print(sub.shape)\n","    print(sub.iloc[0][CFG.CLASSES].sum())\n","    \n","    return sub\n","    \n","sub = make_submission(test_preds_arr, IDs)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### END\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8068726,"sourceId":70203,"sourceType":"competition"},{"datasetId":4745108,"sourceId":8048860,"sourceType":"datasetVersion"},{"datasetId":4746211,"sourceId":8048895,"sourceType":"datasetVersion"},{"datasetId":4746338,"sourceId":8048897,"sourceType":"datasetVersion"},{"datasetId":4789213,"sourceId":8108072,"sourceType":"datasetVersion"},{"datasetId":4836781,"sourceId":8172274,"sourceType":"datasetVersion"},{"datasetId":4941521,"sourceId":8319412,"sourceType":"datasetVersion"},{"datasetId":4963056,"isSourceIdPinned":true,"sourceId":8353087,"sourceType":"datasetVersion"},{"datasetId":4963066,"isSourceIdPinned":true,"sourceId":8353101,"sourceType":"datasetVersion"},{"datasetId":4984448,"isSourceIdPinned":true,"sourceId":8418579,"sourceType":"datasetVersion"},{"datasetId":4963063,"sourceId":8449682,"sourceType":"datasetVersion"},{"sourceId":181443923,"sourceType":"kernelVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":31.037946,"end_time":"2024-04-13T08:15:36.967243","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-13T08:15:05.929297","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
